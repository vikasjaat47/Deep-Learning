{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T05:57:44.413142Z",
     "start_time": "2021-04-24T05:57:44.389770Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T13:08:16.761415Z",
     "start_time": "2021-04-24T13:08:16.669243Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a class Ml to create a Deep learning model based on different conditions\n",
    "\n",
    "class DeepLearning:\n",
    "    \n",
    "    def __init__(self, l_dims, activation, max_epochs = 1000, learning_rate = 0.0075, \n",
    "                 tolerance = 0.00000001, leaky_para = 0.01, cost_type = \"cross entropy\", \n",
    "                 model_type = \"Binary classification\"):\n",
    "        \n",
    "        '''\n",
    "        Parameters: l_dims --> list containing hidden unit of each Neural Network layer\n",
    "                           Note: 1. Neural Network layer does not include input layer.\n",
    "                                 2. Input layer unit will be added in l_dims in \"train\" function later  \n",
    "                                 \n",
    "                    activation --> list containing the type of activation function used at each Neural Network layer\n",
    "                           e.g.: \"relu\", \"leaky_relu\", \"sigmoid\", \"tanh\".  \n",
    "                                             \n",
    "                    max_epochs --> int, how many times the model should go through each training sample.\n",
    "                           Default value --> 1000\n",
    "                           Note: max_iter = max_epochs*batch_size\n",
    "                           \n",
    "                    learning_rate --> decide how much of a step size is taken in updating the parameters \n",
    "                           Default value --> 0.0075         \n",
    "                           \n",
    "                    tolerance --> float value, if abs(prev_cost - current_cost) < tolerance then stop training model\n",
    "                           Default value --> 0.00001\n",
    "                    \n",
    "                           \n",
    "                    leaky_para --> float value, used when activation function is leaky_relu.\n",
    "                           Default value --> 0.01       \n",
    "                           \n",
    "                    cost_type --> type of cost function is used to calculate loss function during training model\n",
    "                           Note: 1. loss function --> defined on one training sample\n",
    "                                 2. cost function --> computed over the batch sample\n",
    "                                 3. Default value --> \"cross entropy\"\n",
    "                                 4. other possible value --> \"multi_class cross entropy\", \"MSE\", \"MAE\".  \n",
    "                                 \n",
    "                    model_type --> type of problem\n",
    "                           Note: 1. Default value --> \"Binary classification\"\n",
    "                                 2. other possible value --> \"Multi_class\", \"Regression\".\n",
    "                                 \n",
    "                    params --> dictionary to keep track of Weights and bais of the model.\n",
    "                    linear_model --> dictionary to keep track of Z's of forward propogation.\n",
    "                           Note: Z = W*A + b (genral form)\n",
    "                    activation_model --> dictionary to keep track of A's of forward propogation.\n",
    "                           Note: A = activation_function(Z)\n",
    "                    grads --> dictionary to keep track of derivatives computed in backward propogation.\n",
    "                           Note: dA's, dZ's, dW's and db's.\n",
    "                    \n",
    "                    \n",
    "        '''\n",
    "        \n",
    "        self.l_dims = l_dims\n",
    "        self.activation = activation\n",
    "        self.max_epochs = max_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.leaky_para = leaky_para\n",
    "        self.cost_type = cost_type\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        self.params = {}  \n",
    "        self.linear_model = {}\n",
    "        self.activation_model = {}\n",
    "        self.grads = {}\n",
    "        \n",
    "        \n",
    "    # helper function to calculate activation function values for each layer\n",
    "    # 1. relu function\n",
    "    def ReLU(self, z):\n",
    "        \n",
    "        # it return the maximum of 0 and z\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    # 2. leaky_ReLU function\n",
    "    def leaky_ReLU(self, z):\n",
    "        \n",
    "        # return z if z >0\n",
    "        #       else returns leaky_para*z \n",
    "        # where leaky_para is user defined and can take a default value of 0.01\n",
    "        \n",
    "        return np.maximum(self.leaky_para*z, z)\n",
    "    \n",
    "    # 3. sigmoid function\n",
    "    def sigmoid(self, z):\n",
    "        \n",
    "        # sigmoid(z) = 1/(1+e(-z))\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    # gradient of activation functions\n",
    "    def grad_activation(self, z, activation):\n",
    "        \n",
    "        # given the type of activation function computed its derivative\n",
    "        if activation == 'relu':\n",
    "            \n",
    "            # relu = max(0, z) so for z>0, derivative = 1, else derivative = 0.\n",
    "            z[z>0.0] = 1\n",
    "            z[z<=0.0] = 0\n",
    "            \n",
    "            return z\n",
    "        \n",
    "        elif activation == \"sigmoid\":\n",
    "            \n",
    "            # sigmoid(z) = 1/(1+exp(-x))\n",
    "            # dreivative of sigmoid(z) = sigmoid(z)*(1-sigmoid(z))\n",
    "            return self.sigmoid(z)*(1.0 - self.sigmoid(z))\n",
    "        \n",
    "        elif activation == \"tanh\":\n",
    "            \n",
    "            # tanh(z) = (exp(z) - exp(-z))/(exp(z)+exp(-z))\n",
    "            # derivative of tanh(z) :- (1 - tanh(z)^2)\n",
    "            return 1.0 - np.power(np.tanh(z), 2)\n",
    "        \n",
    "        else: # for leaky_relu activation function\n",
    "            \n",
    "            # leaky_relu = max(leaky_para*z, z), so for z>0 derivative = 1, else, it is = leaky_para\n",
    "            \n",
    "            z[z>0.0] = 1\n",
    "            z[z<=0.0] = self.leaky_para\n",
    "            \n",
    "            return z\n",
    "            \n",
    "    \n",
    "    # parameter initlization\n",
    "    def params_set(self):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                        l_dims --> now it contains the units of each layer including input layer\n",
    "                               Note: input layer units are added at 0th index in \"train\" function.\n",
    "                                     This function \"params_set\" is called within \"train\" function.\n",
    "                                     \n",
    "                        params --> to store the Initilized value of each layers weights and bais\n",
    "                              Note: 1. shape of W[l] = (l_dims[l], l_dims[l-1])\n",
    "                                    2. shape of b[l] = (l_dims[l], 1)\n",
    "                                    3. l --> 1,2,3,...,L where L dentoes the output layer\n",
    "                                    4. multipled Wl's with 0.01 so that model can train properly\n",
    "        '''\n",
    "        \n",
    "        for l in range(1, len(self.l_dims)):\n",
    "            \n",
    "            self.params[\"W\"+str(l)] = np.random.randn(self.l_dims[l], self.l_dims[l-1])*0.1\n",
    "            self.params[\"b\"+str(l)] = np.zeros((self.l_dims[l], 1))\n",
    "            #print(\"W\"+str(l), \" = \", self.params[\"W\"+str(l)])\n",
    "            #print(\"b\"+str(l), \" = \", self.params[\"b\"+str(l)])\n",
    "        \n",
    "    \n",
    "    # forward propogation\n",
    "    def forward(self, X):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                        l_dims --> used just to iterate over the layers \n",
    "                        params --> used to calculate Z \n",
    "                        linear_model --> to keep track of Z\n",
    "                        activation --> to get the type of activation function used at each layer\n",
    "                        activation_model --> to keep track of A\n",
    "                    \n",
    "                    X --> training set\n",
    "                        Note: stored in activation_model as \"A0\"\n",
    "                              shape of X = (l_dims[0], batch_size)\n",
    "                              l_dims[0] = n_features of X\n",
    "                              \n",
    "                        formula -->      \n",
    "                                   Z[l] = W[l]*A[l-1] + b[l]\n",
    "                                   A[l] = activation_function(Z[l])\n",
    "                                   shape of Z[l] = (l_dims[l], batch_size)\n",
    "                                   shape of A[l] = (l_dims[l], batch_size) \n",
    "        '''\n",
    "\n",
    "        self.activation_model[\"A0\"] = X\n",
    "        for l in range(1, len(self.l_dims)): \n",
    "            \n",
    "            #print(\"A\"+str(l-1), \" = \", self.activation_model[\"A\"+str(l-1)])\n",
    "            \n",
    "            self.linear_model[\"Z\"+str(l)] = np.dot(self.params[\"W\"+str(l)], self.activation_model[\"A\"+str(l-1)]) + self.params[\"b\"+str(l)]\n",
    "            if self.activation[l-1] == 'relu':\n",
    "                \n",
    "                self.activation_model[\"A\"+str(l)] = self.ReLU(self.linear_model[\"Z\"+str(l)])\n",
    "                \n",
    "            elif self.activation[l-1] == 'sigmoid':\n",
    "                \n",
    "                self.activation_model[\"A\"+str(l)] = self.sigmoid(self.linear_model[\"Z\"+str(l)])\n",
    "                \n",
    "            elif self.activation[l-1] == 'tanh' :\n",
    "                \n",
    "                self.activation_model[\"A\"+str(l)] = np.tanh(self.linear_model[\"Z\"+str(l)])\n",
    "                \n",
    "            else:  # activation = 'leaky_relu'\n",
    "                \n",
    "                self.activation_model[\"A\"+str(l)] = self.leaky_ReLU(self.linear_model[\"Z\"+str(l)])\n",
    "                \n",
    "            #print(\"activation function = \", self.activation[l-1])\n",
    "                \n",
    "    \n",
    "    # cost of the model and dAL            \n",
    "    def cost_and_dAL(self, Y):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self:\n",
    "                        activation_model --> to get the output layer A (AL)\n",
    "                        grads --> to store dAL (derivative of cost function w.r.t AL)\n",
    "                        l_dims --> to access the AL\n",
    "                    Y --> used in calculating cost function\n",
    "                    formulas:\n",
    "                           1. cost_type --> \"cross entropy\"\n",
    "                               cost = (-1/m)*sum(Ylog(AL) + (1-Y)log(1-AL))\n",
    "                               \"dA[L]\" --> d(cost)/dA[L] = (-Y/AL) + (1-Y)/(1-AL)\n",
    "                               \n",
    "                           2. cost_type --> \"multi_class cross entropy\"\n",
    "                               cost = (-1/m)*sum(Ylog(AL))\n",
    "                               \"dAL\" = (-Y/AL)\n",
    "                               \n",
    "                           3. cost_type --> \"MSE\" (mean squre error)\n",
    "                               cost = (-1/m)*sum((Y-AL)^2)\n",
    "                               \"dAL\" = -2*(Y-AL)\n",
    "                               \n",
    "                           4. cost_type --> \"MAE\" (mean absolute error)\n",
    "                               cost = (-1/m)*sum(abs(Y-AL))\n",
    "                               \"dAL\" = (AL-Y)/(abs(Y-AL))\n",
    "                               \n",
    "        Returns:  cost function value for a given cost_type\n",
    "        '''\n",
    "        \n",
    "        m = Y.shape[1]\n",
    "        AL = self.activation_model[\"A\"+str(len(self.l_dims)-1)] \n",
    "        #print(\"m = \", m)\n",
    "        #print(\"Y = \", Y)\n",
    "        print(\"cost_type = \", self.cost_type)\n",
    "        \n",
    "        # for binary cross entropy or cross entropy\n",
    "        if self.cost_type == \"cross entropy\": \n",
    "            \n",
    "            cost = (-1/m)*(np.dot(Y, np.log(AL).T)+np.dot(1-Y, np.log(1-AL).T))\n",
    "            self.grads[\"dA\"+str(len(self.l_dims)-1)] = -1*np.divide(Y, AL) + np.divide(1-Y, 1-AL)\n",
    "            cost = np.squeeze(cost)    # if by chance cost is not a value and an array, np.squeeze is used. \n",
    "            \n",
    "        # for multi-class cross entropy\n",
    "        elif self.cost_type == \"multi_class cross entropy\":\n",
    "            \n",
    "            cost = (-1/m)*np.sum(np.multiply(Y, np.log(AL)))\n",
    "            self.grads[\"dA\"+str(len(self.l_dims)-1)] = -Y/AL\n",
    "            cost = np.squeeze(cost)\n",
    "            \n",
    "        # for mean squared error\n",
    "        elif self.cost_type == \"MSE\":\n",
    "            \n",
    "            cost = (-1/m)*np.sum((Y-AL)*(Y-AL))\n",
    "            self.grads[\"dA\"+str(len(self.l_dims)-1)] = -2*(Y-AL)\n",
    "            cost = np.squeeze(cost)\n",
    "            \n",
    "#         # for root mean squared error\n",
    "#         elif self.cost_type = \"RMSE\":\n",
    "            \n",
    "#             cost = np.sqrt((1/m)*np.sum(np.power(Y-AL, 2)))\n",
    "#             self.grads[\"\"dA\"+str(len(self.l_dims)-1)\"] = \n",
    "        # for mean absolute error\n",
    "        else:\n",
    "            \n",
    "            cost = (-1/m)*np.sum(np.abs(Y-AL))\n",
    "            self.grads[\"dA\"+str(len(self.l_dims)-1)] = (AL-Y)/(np.abs(Y-AL))\n",
    "            cost = np.squeeze(cost)\n",
    "            \n",
    "        #print(\"cost = \", cost)\n",
    "        #print(\"dA\"+str(len(self.l_dims)-1), \" = \", self.grads[\"dA\"+str(len(self.l_dims)-1)])\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    # back propogation\n",
    "    def backward(self):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                        batch_size --> used in calculating grads\n",
    "                        l_dims --> to  iterate over each layer\n",
    "                        grads --> to store derivatives \n",
    "                        linear_model, activation_model, params --> used to calculate grads\n",
    "                        \n",
    "                Formulas:\n",
    "                         1. \"dZ[l]\" --> d(cost)/dZ[l] --> (d(cost)/dA[l])*(dA[l]/dZ[l]) --> \"dA[l]\"*(dA[l]/dZ[l])\n",
    "                                      A[l] = activation_function(Z[l])\n",
    "                                      \"dZ[l]\" = \"dA[l]\"*grad_activation(Z[l])   \n",
    "                                      # element-wise product because\n",
    "                                      shape of \"dZ[l]\" --> shape of Z[l] --> shape of A[l] --> shape of \"dA[l]\" \n",
    "                         \n",
    "                         \n",
    "                         2. \"dW[l]\" --> d(cost)/dW[l] --> (d(cost)/dZ[l])*(dZ[l]/dW[l]) --> \"dZ[l]\"*(dZ[l]/dW[l])\n",
    "                                      Z[l] = W[l]*A[l-1] + b[l]\n",
    "                                      # these dW[l] are calculated over batch_size and not after each training sample,\n",
    "                                      # that's why we need to divide it by batch_size\n",
    "                                      \"dW[l]\" = \"dZ[l]\"*A[l-1] # this is genral form \n",
    "                                      # correct formula:\n",
    "                                                      \"dW[l]\" = (1/batch_size)*np.dot(\"dZ[l]\" , A[l-1].T)\n",
    "                                                      shape of \"dW[l]\" --> shape of W[l] --> (l_dims[l], l_dims[l-1])\n",
    "                                                      shape of \"dZ[l]\" --> shape of Z[l] --> (l_dims[l], batch_size)\n",
    "                                                      shape of A[l-1].T --> shape of Z[l-1].T --> (batch_size, l_dims[l-1])\n",
    "                         \n",
    "                         \n",
    "                         3. \"db[l]\" --> d(cost)/db[l] --> (d(cost)/dZ[l])*(dZ[l]/db[l]) --> \"dZ[l]\"*(dZ[l]/db[l])\n",
    "                                      Z[l] = W[l]*A[l-1] + b[l]\n",
    "                                      # Similarly this is also calculated over batch_size\n",
    "                                      \"db[l]\" = sum(\"dZ[l]\")/batch_size        # sum along rows\n",
    "                                      # shape of \"db[l]\" --> shape of b[l] --> (l_dims[l], 1)\n",
    "                                      # shape of \"dZ[l]\" --> shape of Z[l] --> (l_dims[l], batch_size)\n",
    "                                      \n",
    "                        4. \"dA[l-1]\" --> d(cost)/dA[l-1] --> (d(cost)/dZ[l])*(dZ[l]/dA[l-1]) --> \"dZ[l]\"*(dZ[l]/dA[l-1])\n",
    "                                      \"dA[l-1]\" = \"dZ[l]\"*W[l]\n",
    "                                      # above formula is just a representation\n",
    "                                      # correct formula:\n",
    "                                                      \"dA[l-1]\" = np.dot(W[l].T, \"dZ[l]\")\n",
    "                                                      shape of \"dA[l-1]\" --> shape of A[l-1] --> (l_dims[l-1], batch_size)\n",
    "                                                      shape of W[l].T --> (l_dims[l-1], l_dims[l])\n",
    "                                                      shape of \"dZ[l]\" --> shape of Z[l] --> (l_dims[l], batch_size)\n",
    "                         \n",
    "        '''\n",
    "        \n",
    "        m = self.activation_model[\"A\"+str(len(self.l_dims)-1)].shape[1]\n",
    "        for l in reversed(range(1, len(self.l_dims))):\n",
    "            \n",
    "            self.grads[\"dZ\"+str(l)] = self.grads[\"dA\"+str(l)]*self.grad_activation(self.linear_model[\"Z\"+str(l)],\n",
    "                                                                                             self.activation[l-1])\n",
    "            \n",
    "            self.grads[\"dW\"+str(l)] = (1/m)*np.dot(self.grads[\"dZ\"+str(l)], self.activation_model[\"A\"+str(l-1)].T)\n",
    "            \n",
    "            self.grads[\"db\"+str(l)] = (1/m)*np.sum(self.grads[\"dZ\"+str(l)], axis = 1, keepdims= True)\n",
    "            \n",
    "            self.grads[\"dA\"+str(l-1)] = np.dot(self.params[\"W\"+str(l)].T, self.grads[\"dZ\"+str(l)])\n",
    "            \n",
    "            #print(\"dA\"+str(l), \" = \", self.grads[\"dA\"+str(l)])\n",
    "            #print(\"dZ\"+str(l), \" = \", self.grads[\"dZ\"+str(l)])\n",
    "            #print(\"dW\"+str(l), \" = \", self.grads[\"dW\"+str(l)])\n",
    "            #print(\"db\"+str(l), \" = \", self.grads[\"db\"+str(l)])\n",
    "            \n",
    "            \n",
    "    # updating the parameters Wl's and bl's        \n",
    "    def update_params(self):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                        l_dims --> to iterate over each layer\n",
    "                        params, learning_rate, grads --> to update the parameter of the model\n",
    "                        \n",
    "                    Formula:\n",
    "                           W[l] = W[l] - learning_rate*\"dW[l]\"\n",
    "                           b[l] = b[l] - learning_rate*\"db[l]\"\n",
    "        '''\n",
    "        \n",
    "        for l in range(1, len(self.l_dims)):\n",
    "            \n",
    "            #print(\"before W\"+str(l), \" = \", self.params[\"W\"+str(l)])\n",
    "            #print(\"before b\"+str(l), \" = \", self.params[\"b\"+str(l)])\n",
    "            \n",
    "            self.params[\"W\"+str(l)] -= self.learning_rate*self.grads[\"dW\"+str(l)]\n",
    "            self.params[\"b\"+str(l)] -= self.learning_rate*self.grads[\"db\"+str(l)]\n",
    "            \n",
    "            #print(\"after W\"+str(l), \" = \", self.params[\"W\"+str(l)])\n",
    "            #print(\"after b\"+str(l), \" = \", self.params[\"b\"+str(l)])\n",
    "    \n",
    "    \n",
    "    # training the model to get optimum values of Wl's and bl's\n",
    "    def train(self, X, Y):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                        l_dims, params_set, forward, cost_and_dAL, backward, update_params --> to train the model\n",
    "                    X --> whole training set\n",
    "                    Y --> true label set\n",
    "                    shape of X --> (no. of training examples, no. of features)\n",
    "                    shape of Y --> (no. of training examples, 1)\n",
    "                    \n",
    "        '''\n",
    "        \n",
    "        X = X.T                             # to change the shape of X to (no. of features , no. of training example)\n",
    "        Y = Y.T.reshape(1, Y.shape[0])      # to change the shape of Y to (1, no. of training ex.) \n",
    "        n_x = X.shape[0]                    # no. of features of X or dimension of zeroth layer or input layer\n",
    "        self.l_dims.insert(0, n_x)          # inserting the n_x at i = 0 position in l_dims list.\n",
    "        M = X.shape[1]                      # total no. of training example\n",
    "        self.params_set()\n",
    "        costs = []\n",
    "        epochs = []\n",
    "        #print(\"shape of X = \", X.shape)\n",
    "        #print(\"shape of Y = \", Y.shape)\n",
    "        for epoch in range(self.max_epochs):       \n",
    "            epochs.append(epoch)\n",
    "            self.forward(X)\n",
    "            cost = self.cost_and_dAL(Y)\n",
    "            self.backward()\n",
    "            self.update_params()\n",
    "                \n",
    "            costs.append(cost)\n",
    "            print(cost)\n",
    "            \n",
    "            if len(costs) > 2 and abs(cost-costs[-2])< self.tolerance:\n",
    "                break\n",
    "                \n",
    "     \n",
    "         \n",
    "    # predicting the training_set\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # print(\"predicting the output labels ! \\n\")\n",
    "        # used to predict the output labels from model\n",
    "        # calculate AL(output layer result)\n",
    "        # input :- self, X\n",
    "        #       X :- data on which we need to calculate output label with shape (m, n_x)\n",
    "        #       so convert it into (n_x, m) matrix before use\n",
    "        #       forward function to compute Zl's and Al's\n",
    "        #       activation_model an n_layer to get output layer \n",
    "        #       model_type to get final predicated output of the sample\n",
    "        \n",
    "        X = X.T\n",
    "        m = X.shape[1]\n",
    "        # print(\"final parameter for learning are : \\n\", self.params)\n",
    "        self.forward(X)\n",
    "        AL = self.activation_model[\"A\"+str(len(self.l_dims)-1)]\n",
    "        # print(\"final layer values = \\n\", AL)\n",
    "        \n",
    "        # for Binary classification\n",
    "        if self.model_type:\n",
    "            \n",
    "            #print(\"type of model = \", self.model_type, \"\\n\")\n",
    "            # for binary classification we used sigmoid model at output layer\n",
    "            # so our values are between 0 and 1\n",
    "            # so if value > 0.05 output = 1\n",
    "            # else , output = 0\n",
    "            \n",
    "            AL[AL>0.5] = 1\n",
    "            AL[AL<= 0.5] = 0\n",
    "        \n",
    "        # for Multi_class classification\n",
    "        elif self.model_type == \"Multi_class\":\n",
    "            \n",
    "            # print(\"type of model = \", self.model_type, \"\\n\")\n",
    "            # shape of AL(output layer) = (n_class, m_example) where n_class is given as an input while calling the class DeepLearning\n",
    "            # shape of Y(true output) = (n_class, m_example) \n",
    "            # but we want our output/y_hat to be (1, m_example) where in each column we will store the index of the \n",
    "            # maximum element in each column of AL\n",
    "            \n",
    "            AL = np.argmax(AL, axis = 0)\n",
    "        # print(\"output label \\n\", AL)    \n",
    "        # for Regression\n",
    "        # no need to change anything since it is the expected output we wanted\n",
    "        # last layer will never use sigmoid, it will use ReLU or leaky_ReLU functions to calculate AL\n",
    "        \n",
    "        return AL.reshape(m, 1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
