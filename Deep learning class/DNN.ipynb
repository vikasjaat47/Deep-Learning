{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-26T17:23:04.248469Z",
     "start_time": "2021-04-26T17:23:03.871671Z"
    }
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important steps involved are: -\n",
    "\n",
    "                           0. __init__ function\n",
    "                           1. Parameter_Initialization\n",
    "                           2. Forward_Prop\n",
    "                           3. Cost_dAL_and_dZL\n",
    "                           4. Back_Prop\n",
    "                           5. Parameter_Update\n",
    "                           6. fit\n",
    "                           7. predict\n",
    "                           8. gred_checking\n",
    "                    \n",
    "Some helper function used in some of these steps are: -\n",
    "\n",
    "    1. Activation functions: -\n",
    "                           1) sigmoid \n",
    "                           2) relu\n",
    "                           3) leaky_relu\n",
    "                           4) tanh (used directly in place using np.tanh() function)\n",
    "                           5) softmax\n",
    "                           6) linear\n",
    "                           \n",
    "    2. Activation gradient function\n",
    "    \n",
    "More helper function which are not part of the learning model per say but can be a part of learning model:-\n",
    "\n",
    "    1. params_to_vector       --> to convert W's and b's into column vector each and concatenate them together .\n",
    "    2. grads_to_vector        --> to convert dW's and db's into column vector each and concatenate them together.\n",
    "    3. vectors_to_params      --> after updating the concatenated vectors(theta+epsilon), (theta-epsilon), convert them back                                     into W's and b's to perform Forward_Prop to compute cost\n",
    "    4. grad_checking          --> to check if gradient descent is working properly and no mistakes were made during                                               Forward_Prop or Back_Prop. \n",
    "    5. Random_mini_batches    --> convert X and Y into randomly shuffled mini_batches.\n",
    "    6. decay                  --> updating (decreasing) learning_rate after a fix interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params_to_vector:-\n",
    "\n",
    "                      Given W[l] and b[l] for each layer, convert them into column vectors\n",
    "                      and concatenate them together on top of each other.\n",
    "                      result --> [[W1],[b1],...[WL],[bL]]\n",
    "                             --> here W[1] is also a column vector\n",
    "                             --> shape of result = (something, 1) --> column vector\n",
    "                             \n",
    "### grads_to_vector:-\n",
    "\n",
    "                      Given dW[l] and db[l] for each layer, convert them into column vectors\n",
    "                      and concatenate them together on top of each other.\n",
    "                      result --> [[dW1],[db1],...[dWL],[dbL]]\n",
    "                             --> [[dJ/dtheta1], [dJ/dtheta2], .....]\n",
    "                             --> here W[1] is also a column vector\n",
    "                             --> shape of result = (something, 1) --> column vector --> this is my dJ/dtheta = grad\n",
    "                             \n",
    "### vectors_to_params:-\n",
    "                \n",
    "                      Gievn W[l], b[l] stacked vertically\n",
    "                      convert them into their original form\n",
    "                      \n",
    "### grad_checking:-\n",
    "                    \n",
    "                    theta = params_to_vector\n",
    "                    theta_plus = theta + epsilon\n",
    "                    params_plus = vectors_to_params(theta_plus)\n",
    "                    theta_minus = theta - epsilon\n",
    "                    param_minus = vectors_to_params(theta_minus)\n",
    "                    J_plus = Forward_prop(params_plus) followed by --> Cost_dAL_and_dWL(params_plus)\n",
    "                    J_minus = Forward_prop(params_minus) followed by --> Cost_dAL_and_dWL(params_minus)\n",
    "                    grad_approx = (J_plus - J_minus)/(2*epsilon)\n",
    "                    grad = [[dJ/dtheta1], [dJ/dtheta2],....]\n",
    "                    \n",
    "                    if norm(grad_approx - grad)/(norm(grad) + norm(grad_approx)) > 2*epsilon\n",
    "                        --> There is a mistake in Back_Prop()\n",
    "                    \n",
    "### Random_mini_batches:-\n",
    "\n",
    "                   Given mini_batch_size, X, and Y\n",
    "                      --> randomly shuffle X and Y\n",
    "                      --> get how many batches can be formed with batch size = mini_batch_size\n",
    "                            --> get those mini_batches\n",
    "                      --> if there are still some training examples left\n",
    "                            --> get those too\n",
    "                            \n",
    "### decay:-\n",
    " \n",
    "                   Reduces the learning rate of the model at a fixed interval\n",
    "                      --> new learning_rate = old_learning_rate/(1 + decay_rate*(epoch_num/fixed_interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter_Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing/Exploding gradient: -\n",
    "\n",
    "       During Initialization if Weights and bais are big, they might cause the dW's or db's to be either \n",
    "       very big or very small, effecting the learning of the model.\n",
    "       To avoid it, Initialization is done using \"he\" or \"Xavier\" Initialization.\n",
    "       \n",
    "               1. \"he\" Initialization:- \n",
    "                                       factor = sqrt(2/l_dims[l-1])  for layer l\n",
    "                                       here l_dims[l-1] is # hidden unit for hidden layer l\n",
    "                                       Note:- Used when activation function of the lth layer is \"relu\" or \"leaky_relu\"\n",
    "\n",
    "                2. \"Xavier\" Initialization:-\n",
    "                               factor = sqrt(1/l_dims[l-1]) for layer l\n",
    "                               Note:- Used when activation function of the lth layer is \"tanh\" or \"sigmoid\" or \"softmax\"\n",
    "                               \n",
    "                use these factor to scale Weights of the model.\n",
    "                shape of W[l] --> (l_dims[l], l_dims[l-1])\n",
    "                shape of b[l] --> (l_dims[l], 1)\n",
    "    \n",
    "    \n",
    "### Batch_norm (optional) :-\n",
    "\n",
    "            Normalize Zorig (Z[l] = W[L]*A[l-1] + b[l]) so that our model can train faster\n",
    "            \n",
    "            Z_norm = (Zorig - mean(Zorig))/(sqrt(variance + epsilon))\n",
    "            Z = gama_norm*Z_norm + beta_norm\n",
    "            Note:- 1. epsilon is added to avoid division by zero.\n",
    "                   2. gama_norm*Z_norm is element-wise product and not a dot product because dot product\n",
    "                      will sum up values while element-wise product will only scale up/down which is what we need.\n",
    "            \n",
    "            for batch_norm we need gama_norm matrix and beta_norm vector.\n",
    "               shape of gama_norm:-\n",
    "                             shape of Z = (l_dims[l], mini_batch_size)\n",
    "                             gama_norm will be a column vector of shape (l_dims[l], 1)\n",
    "                       \n",
    "               shape of beta_norm:- \n",
    "                             beta_norm is a vector so, shape of beta_norm --> (l_dims[l], 1)\n",
    "                             \n",
    "            if batch_norm == True:\n",
    "                       Initialize the gama_norm and beta_norm as Weights and bais and they will get updated\n",
    "                       similarly in the back propagation\n",
    "\n",
    "\n",
    "### learnin_algo (optional) :-\n",
    "                  \n",
    "               if learning algorithm = \"momentum\" or \"Adam\"\n",
    "                   we need to Initialize V_dW and V_db to zero matrix and zero vector for each layer\n",
    "                   they will get updated after each epoch\n",
    "                   shape of V_dW = shape of dW = shape of W = [l_dims[l], l_dims[l-1])\n",
    "                   shape of V_db = shape of b = [l_dims[l], 1)\n",
    "                   \n",
    "               if learning algorithm = \"RMSprop\" or \"Adam\"\n",
    "                   Initialize S_dW and S_db to zero matrix and zero vector for each layer\n",
    "                   they will be updated after each epoch\n",
    "                   shape of S_dW = shape of W\n",
    "                   shape of S_db = shape of b\n",
    "                   \n",
    "                   \n",
    "### Dropout (keep_prob != 1) :-\n",
    "          \n",
    "                Initialize D[l] for each layer which takes values as 0 and 1\n",
    "                    --> 0 being that \"neuron\" is shut down\n",
    "                    --> 1 being that \"neuron\" is not shut down during learning\n",
    "                    --> shape of D[l] = shape of Z[l] = shape of A[l] --> (l_dims[l], mini_batch_size)\n",
    "                    \n",
    "                    D[l] = np.random.rand(l_dims[l], mini_batch_size)\n",
    "                           --> create a random matrix \n",
    "                    D[l] = (D[l] < keep_prob).astype(int) \n",
    "                           --> keep_prob*100 % of the total values of D[l] are kept 1 else are 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward_Prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear_model:- \n",
    "              \n",
    "              stores the linear calculated part of the model.\n",
    "              Z[l] = W[l]*A[l-1] + b[l], l = 1,2,...,L\n",
    "              and A[0] = X\n",
    "              \n",
    "### Activation_model:-\n",
    "               \n",
    "              stores the values after applying activation function on Z\n",
    "              A[l] = activation(Z[l])\n",
    "              \n",
    "### batch_norm:-\n",
    "\n",
    "              if set to True, normalize Z[l] as described in \"Parameter_Initialization\" step.\n",
    "            \n",
    "### Dropout (keep_prob != 1):-\n",
    "\n",
    "              A[l] = A[l].*D[l]\n",
    "                    --> .* is element-wise product\n",
    "              A[l] = A[l]/keep_prob\n",
    "                    --> element-wise devision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost_dAL_and_dZL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost:-\n",
    "      \n",
    "       Cost = J(W[1], b[1], ...W[L],b[L]) + Regularization_cost\n",
    "       for different type of model (\"binary\", \"multi\", \"reg\"), different formula is used to calculate J\n",
    "       \n",
    "       model = \"binary\":\n",
    "              J = (-1/mini_batch_size)*(Sum(Y_true*log(AL) + (1-Y_true)*log(1-AL))\n",
    "              here AL is output layer post activation value\n",
    "              \n",
    "       model = \"multi\":\n",
    "              J = (-1/mini_batch_size)*sum(Y_true*log(AL))\n",
    "              \n",
    "       model = \"reg\":\n",
    "              cost_reg = \"MSE\":\n",
    "                       J = (1/mini_batch_size)*sum((Y_true-AL)^2)\n",
    "                       \n",
    "              cost_reg = \"MAE\":\n",
    "                       J = (1/mini_batch_size)*sum(|Y_true - AL|)\n",
    "                       here |Y_true - AL| = abs(Y_true - AL)\n",
    "                       \n",
    "      Regularization_cost does not depend upon model and cost_reg\n",
    "      \n",
    "      Regularization_cost  = lambd*sum(W[l]^2)/(2*mini_batch_size)\n",
    "           frobenius_norm = sqrt(sum(W^2)) --> sum element_wise square of W \n",
    "                   val1 = np.linalg.norm(W)\n",
    "                   val2 = np.sqrt(np.sum(W^2))\n",
    "                   both val1 and val2 are same thing. --> val1 = val2 = frobenius_norm\n",
    "           sum(W[l]^2) --> sum(frobenius_norm^2) --> sum of element-wise square of W for each layer \n",
    "            \n",
    "### \"dAL\" and \"dZL\" :-\n",
    "\n",
    "          \"dAL\" = d(cost)/dAL = dJ/dAL + d(Regularization_cost)/dAL --> dJ/dAL\n",
    "          \"dZL\" = d(cost)/dZL = dJ/dZL + d(Regularization_cost)/dZL  --> dJ/dZL\n",
    "               --> \"dZL\" = (dJ/dAL)*(dAL/dZL) --> \"dAL\"*grad_activation(Z[L])\n",
    "               --> A[L] = AL = activation(Z[L]) --> dAL/dZL = derivative of activation function wrt Z[L]\n",
    "               \n",
    "           model = \"binary\":\n",
    "                  \"dAL\" = dJ/dAL --> \"dAL\" = (-Y_true/AL) + (1-Y_true)/(1-AL)\n",
    "                  \"dZL\" = \"dAL\"*grad_activation(Z[L])\n",
    "                  \n",
    "           model = \"multi\":\n",
    "                  \"dAL\" = dJ/dAL  --> \"dAL\" = -Y_true/AL\n",
    "                  \"dZL\" = AL - Y\n",
    "                  \n",
    "           model = \"reg\":\n",
    "                    In \"reg\" model, activation function used for output layer is always \"linear\"\n",
    "                    so, grad_activation(Z[L]) = 1\n",
    "                    cost_reg = \"MSE\":\n",
    "                          \"dAL\" = dJ/dAL  --> \"dAL\" = -2*(Y_true - AL)\n",
    "                          \"dZL\" = \"dAL\"*grad_activation(Z[L]) = \"dAL\"\n",
    "                          \n",
    "                    cost_reg = \"MAE\":\n",
    "                          \"dAL\" = dJ/dAL  --> \"dAL\" = (AL - Y_true)/|AL - Y_true| or (AL - Y_true)/abs(AL - Y_true)\n",
    "                          \"dZL\" = dJ/dZL -->  (dJ/dAL)*(dAL/dZL) --> \"dAL\"*(dAL/dZL)\n",
    "                                         --> \"dAL\"*grad_activation(Z[L])\n",
    "                                                       --> grad_activation(Z[L]) = 1\n",
    "                          \"dZL\" = \"dAL\"\n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back_Prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"dZL\" and \"dAL\" calculated from \"Cost_dAL_and_dZL\" function\n",
    " m = mini_batch_size\n",
    " can use Dropout with batch_norm too.\n",
    " \n",
    "### Dropout (keep_prob != 1):-\n",
    " \n",
    "                      apply:-\n",
    "                              \"dA[l]\" = \"dA[l]\".*D[1]\n",
    "                              \"dA[l]\" = \"dA[l]/keep_prob[l-1]\n",
    "                                   --> l goes from 1, 2,.... L\n",
    "                                   --> keep_prob[l-1] is the variable for the l_th layer\n",
    "                             Above formula to if we want to use it with or without batch_norm\n",
    "\n",
    "### without batch_norm:-\n",
    "\n",
    "                         Formula used in Forward_Prop:-\n",
    "                                     --> Z[l] = W[l]*A[l-1] + b[l]\n",
    "                                     --> A[l] = activation(Z[l])\n",
    "                          \n",
    "                         1. \"dW[l]\" = d(cost)/dW[l] = dJ/dW[l] + d(Regularization_cost)/dW[l] \n",
    "                                     --> (dJ/dZ[l])*(dZ[l]/dW[l]) + lambd*W[l]/m\n",
    "                                     --> \"dZ[l]\"*(dZ[l]/dW[l]) + lambd*W[l]/m\n",
    "                                              --> dZ[l]/dW[l] = A[l-1]\n",
    "                                     --> \"dZ[l]\"*A[l-1] + lambd*W[l]/m\n",
    "                                              --> shape of dZ[l] = (l_dims[l], m), shape of A[l-1] = (l_dims[l-1], m)\n",
    "                                              --> shape of dW[l] = shape of W[l] = (l_dims[l], l_dims[l-1])\n",
    "                            \"dW[l]\" = (1/m)*\"dZ[l]\"*A[l-1].T + lambd*W[l]/m  \n",
    "                                     --> * represent dot product\n",
    "                                     --> A[l-1].T is transpose of A[l-1]\n",
    "                                     --> This update is done after the model has seen m examples\n",
    "                                     --> that's why it is averaged over m\n",
    "                                      \n",
    "                         2. \"db[l]\" = d(cost)/db[l] = dJ/db[l] + d(Regularization_cost)/db[l]\n",
    "                                     --> (dJ/dZ[l])*(dZ[l]/db[l]) + 0\n",
    "                                     --> \"dZ[l]\"*(dZ[l]/db[l])\n",
    "                                              --> dZ[l]/db[l] = 1\n",
    "                                     --> \"dZ[l]\"\n",
    "                                              --> shape of dZ[l] = (l_dims[l], m)\n",
    "                                              --> shape of db[l] = shape of b[l] = (l_dims[l], 1)\n",
    "                            \"db[l]\" = sum(\"dZ[l]\")/m\n",
    "                                     --> seen m example so averaged over m.\n",
    "                                     --> sum is done along each row to match the dimension of \"db[l]\"\n",
    "                                      \n",
    "                         3. \"dA[l-1]\" = d(cost/dA[l-1]) = (dJ/dZ[l])*(dZ[l]/dA[l-1]) + d(Regularization_cost)/dA[l-1])\n",
    "                                     --> \"dZ[l]\"*(dZ[l]/dA[l-1]) + 0\n",
    "                                              --> dZ[l]/dA[l-1] = W[l]\n",
    "                                     --> \"dZ[l]\"*W[l]\n",
    "                                              --> shape of \"dZ[l]\" = (l_dims[l], m)\n",
    "                                              --> shape of \"W[l]\" = (l_dims[l], l_dims[l-1])\n",
    "                                              --> shape of \"dA[l-1]\" = shape of A[l-1] = (l_dims[l-1], m)\n",
    "                            \"dA[l-1]\" = (W[l].T)*\"dZ[l]\"\n",
    "                                   --> \"dA[l-1]\" = \"dA[l-1]\".*D[l-1]\n",
    "                                   --> \"dA[l-1]\" = \"dA[l-1]\"/keep_prob[l-2]\n",
    "                                     \n",
    "                         4. \"dZ[l-1]\" = d(cost)/dZ[l-1] = (dJ/dA[l-1])*(dA[l-1]/dZ[l-1]) + d(Regularization_cost/dZ[l-1])\n",
    "                                     --> \"dA[l-1]\"*(dA[l-1]/dZ[l-1]) + 0\n",
    "                                              --> dA[l-1]/dZ[l-1]  = grad_activation(Z[l-1])\n",
    "                                     --> \"dA[l-1]\"*grad_activation(Z[l-1])\n",
    "                                              --> grad_activation(Z[l-1]) is gradient of activation function of (l-1)th layer\n",
    "                                                  with respect to Z[l-1]\n",
    "                                              --> shape of \"dA[l-1]\" = (l_dims[l-1], m)\n",
    "                                              --> shape of grad_activation(Z[l-1]) = shape of A[l-1] = (l_dims[l-1], m)\n",
    "                                              --> shape of \"dZ[l-1]\" = (l_dims[l-1], m)\n",
    "                            \"dZ[l-1]\" = \"dA[l-1]\".*grad_activation(Z[l-1])\n",
    "                                              --> .* represent element-wise dot product\n",
    "                                              \n",
    "                                              \n",
    "### With batch_norm:-\n",
    " \n",
    "                         Formula used in Forward_Prop:-\n",
    "                                     --> Z_orig[l] = W[l]*A[l-1] + b[l]\n",
    "                                     --> Z_norm[l] = (Z_orig[l]  - mean(Z_orig[l]))/(sqrt(var(Z_orig[l]) + epsilon)\n",
    "                                     --> Z[l] = gama_norm[l].*Z_norm[l] + beta_norm[l]\n",
    "                                     --> A[l] = activation(Z[l])\n",
    "                                     \n",
    "                         Last thing calculated will be dZ[l] and dA[l]\n",
    "                         1. \"d_gama_norm[l]\" = d(cost)/d_gama_norm[l]\n",
    "                                              -->  (dJ/dZ[l])*(dZ[l]/d_gama_norm[l]) + d(Regularization_cost)/d_gama_norm[l]\n",
    "                                     --> \"dZ[l]\"*(dZ[l]/d_gama_norm[l]) + 0\n",
    "                                              --> dZ[l]/d_gama_norm[l] = Z_norm[l]\n",
    "                                     --> \"dZ[l]\".*Z_norm[l]\n",
    "                                              --> .* represent element-wise product\n",
    "                                              --> shape of dZ[l] = shape of Z_norm = (l_dims[l], m)\n",
    "                                              --> shape of d_gama_norm[i] = (l_dims[l], 1)\n",
    "                            \"d_gama_norm[l]\" = sum(\"dZ[l]\".*Z_norm[l])/m\n",
    "                                              --> this sum is done along each row\n",
    "                                              --> seen m example so averaged over m\n",
    "                                              \n",
    "                         2. \"d_beta_norm[l]\" =  d(cost)/d_beta_norm[l]\n",
    "                                              -->  (dJ/dZ[l])*(dZ[l]/d_beta_norm[l]) + d(Regularization_cost)/d_beta_norm[l]\n",
    "                                     --> \"dZ[l]\"*(dZ[l]/d_beta_norm[l]) + 0\n",
    "                                              --> dZ[l]/d_beta_norm[l] = 1\n",
    "                                     --> \"dZ[l]\"\n",
    "                                              --> shape of dZ[l] = (l_dims[l], m)\n",
    "                                              --> shape of d_beta_norm[i] = (l_dims[l], 1)\n",
    "                            \"d_gama_norm[l]\" = sum(\"dZ[l]\")/m\n",
    "                                              --> this sum is done along each row\n",
    "                                              --> seen m example so averaged over m\n",
    "                                               \n",
    "                         3. \"dZ_orig[l]\" = d(cost)/dZ_orig[l] = dJ/dZ_orig[l] + d(Regularization_cost)/dZ_orig[l]\n",
    "                                     --> dJ/dZ_orig[l] + 0\n",
    "                            \"dZ_orig[l]\" = gama_norm[l].*{m*\"dZ[l]\" - d_gama_norm[l].*Z_norm[l] -       d_beta_norm[l]*I_m}/(m*sqrt(var(Z_orig[l]) + epsilon)\n",
    "                                     --> .* is element-wise product\n",
    "                                     --> * is dot product\n",
    "                                     --> I_m is a row vector of ones of shape (1, m)\n",
    "                                     \n",
    "                         4. \"dW[l]\" = d(cost)/dW[l] = dJ/dW[l] + d(Regularization_cost)/dW[l] \n",
    "                                     --> (dJ/dZ_orig[l])*(dZ_orig[l]/dW[l]) + lambd*W[l]/m\n",
    "                                     --> \"dZ_orig[l]\"*(dZ_orig[l]/dW[l]) + lambd*W[l]/m\n",
    "                                              --> dZ_orig[l]/dW[l] = A[l-1]\n",
    "                                     --> \"dZ_orig[l]\"*A[l-1] + lambd*W[l]/m\n",
    "                                              --> shape of dZ_orig[l] = (l_dims[l], m), shape of A[l-1] = (l_dims[l-1], m)\n",
    "                                              --> shape of dW[l] = shape of W[l] = (l_dims[l], l_dims[l-1])\n",
    "                            \"dW[l]\" = (1/m)*\"dZ_orig[l]\"*A[l-1].T + lambd*W[l]/m  \n",
    "                                      \n",
    "                         5. \"db[l]\" = d(cost)/db[l] = dJ/db[l] + d(Regularization_cost)/db[l]\n",
    "                                     --> (dJ/dZ_orig[l])*(dZ_orig[l]/db[l]) + 0\n",
    "                                     --> \"dZ_orig[l]\"*(dZ_orig[l]/db[l])\n",
    "                                              --> dZ_orig[l]/db[l] = 1\n",
    "                                     --> \"dZ_orig[l]\"\n",
    "                                              --> shape of dZ_orig[l] = (l_dims[l], m)\n",
    "                                              --> shape of db[l] = shape of b[l] = (l_dims[l], 1)\n",
    "                            \"db[l]\" = sum(\"dZ_orig[l]\")/m\n",
    "                                     --> sum is done along each row to match the dimension of \"db[l]\"\n",
    "                                      \n",
    "                         6. \"dA[l-1]\" = d(cost/dA[l-1])\n",
    "                                              --> (dJ/dZ_orig[l])*(dZ_orig[l]/dA[l-1]) + d(Regularization_cost)/dA[l-1])\n",
    "                                     --> \"dZ_orig[l]\"*(dZ_orig[l]/dA[l-1]) + 0\n",
    "                                              --> dZ_orig[l]/dA[l-1] = W[l]\n",
    "                                     --> \"dZ_orig[l]\"*W[l]\n",
    "                                              --> shape of \"dZ_orig[l]\" = (l_dims[l], m)\n",
    "                                              --> shape of \"W[l]\" = (l_dims[l], l_dims[l-1])\n",
    "                                              --> shape of \"dA[l-1]\" = shape of A[l-1] = (l_dims[l-1], m)\n",
    "                            \"dA[l-1]\" = (W[l].T)*\"dZ_orig[l]\"\n",
    "                                   --> \"dA[l-1]\" = \"dA[l-1]\".*D[l-1]\n",
    "                                   --> \"dA[l-1]\" = \"dA[l-1]\"/keep_prob[l-2]\n",
    "                                     \n",
    "                         7. \"dZ[l-1]\" = d(cost)/dZ[l-1] = (dJ/dA[l-1])*(dA[l-1]/dZ[l-1]) + d(Regularization_cost/dZ[l-1])\n",
    "                                     --> \"dA[l-1]\"*(dA[l-1]/dZ[l-1]) + 0\n",
    "                                              --> dA[l-1]/dZ[l-1]  = grad_activation(Z[l-1])\n",
    "                                     --> \"dA[l-1]\"*grad_activation(Z[l-1])\n",
    "                            \"dZ[l-1]\" = \"dA[l-1]\".*grad_activation(Z[l-1])\n",
    "                                              --> .* represent element-wise dot product\n",
    "                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:39:08.895926Z",
     "start_time": "2021-04-28T12:39:08.871777Z"
    }
   },
   "source": [
    "                         W[l] = W[l] - learning_rate*dW_factor\n",
    "                         b[l] = b[l] - learning_rate*db_factor\n",
    "                                   --> dW_factor and db_factor are calculated based on the learning algorithm used\n",
    "                                   \n",
    "                      \n",
    "### \"gd\" learning Algorithm:-\n",
    "\n",
    "                         Standard gradient descent is applied.\n",
    "                         dW_factor = \"dW[l]\"\n",
    "                         db_factor = \"db[l]\"\n",
    "                         \n",
    "### \"momentum\" learning Algorithm:- \n",
    "        \n",
    "            for each Neural network layer -->\n",
    "                         Initialized V_dW = 0 matrix of shape W, V_db = 0 vector of shape b\n",
    "                         After a pass through a mini_batch\n",
    "                                V_dW = beta1*V_dW + (1-beta1)*dW\n",
    "                                V_db = beta1*V_db + (1-beta1)*db\n",
    "                                dW_factor = V_dW\n",
    "                                db_factor = V_db\n",
    "                                \n",
    "### \"RMSprop\" learning Algorithm:-\n",
    "\n",
    "            for each Neural network layer -->\n",
    "                         Initialized S_dW = 0 matrix of shape W, S_db = 0 vector of shape b\n",
    "                         After a pass through a mini_batch\n",
    "                                S_dW = beta2*S_dW + (1-beta2)*(dW^2)\n",
    "                                S_db = beta2*S_db + (1-beta2)*(db^2)\n",
    "                                dW_factor = dW/(sqrt(S_dW) + epsilon) \n",
    "                                db_factor = db/(sqrt(S_db) + epsilon)\n",
    "                                \n",
    "### \"Adam\" learning Algorithm:-\n",
    "\n",
    "            for each Neural network layer -->\n",
    "                         t = 0 --> \"Adam\" parameter, used in calculation\n",
    "                         Initialized V_dW = 0 matrix of shape W, V_db = 0 vector of shape b\n",
    "                         After a pass through a mini_batch\n",
    "                                t += 1\n",
    "                                V_dW = beta1*V_dW + (1-beta1)*dW\n",
    "                                V_db = beta1*V_db + (1-beta1)*db\n",
    "                                S_dW = beta2*S_dW + (1-beta2)*(dW^2)\n",
    "                                S_db = beta2*S_db + (1-beta2)*(db^2)\n",
    "                                V_dW_corrected = V_dW/(1-beta1^t)\n",
    "                                V_db_corrected = V_db/(1-beta1^t)\n",
    "                                S_dW_corrected = S_dW/(1-beta2^t)\n",
    "                                S_db_corrected = S_db/(1-beta2^t)\n",
    "                                dW_factor = V_dW_corrected/(sqrt(S_dW_corrected) + epsilon)\n",
    "                                db_factor = V_db_corrected/(sqrt(S_db_corrected) + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Mini_batches :-\n",
    " \n",
    "                    --> shuffle X and Y randomly\n",
    "                    --> given the mini_batch_size, get mini_batch_X and mini_batch_Y from Random_mini_batches\n",
    "                    --> Pass this batch through the learning model\n",
    "                    --> update parameter based on learning from this pass\n",
    "                    --> repeat for all the batches of X and Y\n",
    "                    --> This all process is called \"1 epoch\" or \"1 pass\"\n",
    "                    --> train the model for \"num epochs\" or \"max epochs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't Use Batch_norm and Dropout togather\n",
    " \n",
    "                    1. batch_norm is used to speed up learning of the model\n",
    "                    2. batch_norm makes the learning stable\n",
    "                          --> can use large learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T23:54:08.764475Z",
     "start_time": "2021-05-01T23:54:08.520315Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Creating a class Ml to create a Deep learning model based on different conditions\n",
    "import numpy as np\n",
    "import math\n",
    "class DNN:\n",
    "    \n",
    "    def __init__(self, l_dims, activation, mini_batch_size = 64, max_epochs = 3000, learning_rate = 0.001,\n",
    "                 decay_rate = 1.0, decay_interval = 500, is_decay = False, epsilon = 0.00000001, lambd = 0, \n",
    "                 Dropout = False, keep_prob = None, batch_norm = False, cost_reg = \"MSE\", learning_algo = \"gd\", \n",
    "                 beta1 = 0.9, beta2 = 0.999,  leaky_para = 0.01, model_type = \"binary\"):\n",
    "        \n",
    "        '''\n",
    "    Parameters:- \n",
    "                  l_dims --> type --> python list, value_type --> int, \n",
    "                            Note:- 1. contains the # hidden units for hidden layers and # output units for output layers.\n",
    "                                   2. length of l_dims = L (total # hidden layers + output layer)\n",
    "                                   3. later in the \"fit\" method, # input features are added into it at index 0.\n",
    "                                   4. So, throughout the model, we will be considering the length of l_dims = L+1\n",
    "\n",
    "                  activation --> type --> python list, value_type --> str,\n",
    "                            Note:- 1. length of activation = L\n",
    "                                   2. activation[l] --> stores the activation of (l-1)th layer\n",
    "                                   3. values --> \"relu\", \"leaky_relu\", \"sigmoid\", \"tanh\", \"softmax\" and \"linear\".\n",
    "\n",
    "                  mini_batch_size --> hyperparameter, value_type --> int\n",
    "                            Note:- 1. # training sample used before updating the model parameters \n",
    "                                   2. Default value --> 64\n",
    "                                   3. other common values --> 32, 128, 256, 512, 1024 (multiple of two)\n",
    "                                   4. most frequently used values --> 64, 128, 256\n",
    "\n",
    "                  max_epochs --> hyperparameter, value_type --> int\n",
    "                            Note:- 1. # times model trained on each mini_batch\n",
    "                                   2. Default value --> 5000\n",
    "\n",
    "                  learning_rate --> hyperparameter, value_type --> float\n",
    "                            Note:- 1. how much of a step is taken in the downhill direction \n",
    "                                   2. Default value --> 0.001\n",
    "\n",
    "                  decay_rate --> hyperparameter, value_type --> float\n",
    "                            Note:- 1. used in formula to calculate learning_rate decay\n",
    "                                   2. Default value --> 1.0\n",
    "                                   \n",
    "                  decay_interval --> hyperparameter, value_type --> int\n",
    "                            Note:- 1. used in formula to calculate learning_rate decay\n",
    "                                   2. Default value --> 100\n",
    "                                   \n",
    "                  is_decay --> boolean\n",
    "                            Note:- 1. if set to true then \"decay\" function is used and\n",
    "                                      learning rate decreases at fix interval\n",
    "                                   2. Default value --> True  \n",
    "\n",
    "                  epsilon --> hyperparameter, value_type --> float\n",
    "                            Note:- 1. used at different places to avoid dividing by zero\n",
    "                                   2. Default value --> 0.00000001\n",
    "                                   3. used in \"batch_norm\", \"RMSprop\" algorithm and \"Adam\" algorithm (defined below)\n",
    "\n",
    "                  lambd --> hyperparameter, value_type --> float\n",
    "                            Note:- 1. Used in Regularization\n",
    "                                   2. Default value --> 0\n",
    "                                   3. Train the model without regularization, get training_error and dev_error\n",
    "                                   4. dev_error - training_error is large (Overfitting) \n",
    "                                   5. Regularization is one of the techniques used to reduce overfitting\n",
    "                  \n",
    "                  Dropout --> Boolean\n",
    "                            Note:- 1. Default value --> False\n",
    "                                   2. If set to True --> dropout will be implemented\n",
    "                                   3. Dropout is another techniques used to reduce overfitting\n",
    "                  \n",
    "                  keep_prob --> hyperparameter, value_type --> list\n",
    "                            Note:- 1. Default value --> None\n",
    "                                   2. if Dropout is True --> a list will be provided\n",
    "                                   3. list contains float value --> [0, 1)\n",
    "                                   4. defines the fraction of hidden units of each layer to be kept open for training\n",
    "\n",
    "                  batch_norm --> type --> boolean\n",
    "                            Note:- 1. Default value --> False\n",
    "                                   2. if True, normalizes the layer's (hidden layers + output layer) Z values\n",
    "                                   3. batch_norm is done to increase speed of learning\n",
    "\n",
    "                  cost_reg --> type --> str\n",
    "                            Note:- 1. Default value --> \"MSE\"\n",
    "                                   2. Type of cost function used in regression model\n",
    "\n",
    "                  learning_algo --> type --> str\n",
    "                            Note:- 1. Type of learning algorithm is used to update the model parameters\n",
    "                                   2. Default value --> \"gd\" short for standard gradient descent\n",
    "                                   3. other values --> \"momentum\", \"RMSprop\", \"Adam\"\n",
    "\n",
    "                  beta1 --> hyperparameter,  type --> float\n",
    "                            Note:- 1. Default value --> 0.9\n",
    "                                   2. hyperparameter for \"momentum\" learning algorithm\n",
    "                                   3. most of the time it is not used as a hyperparameter and its value is fixed at 0.9\n",
    "\n",
    "                  beta2 --> hyperparameter, type --> float\n",
    "                            Note:- 1. Default value --> 0.999\n",
    "                                   2. hyperparameter for \"RMSprop\" learning algorithm\n",
    "                                   3. most of the time it is not used as a hyperparameter and its value is fixed at 0.999\n",
    "\n",
    "                  leaky_para --> hyperparameter, type --> float\n",
    "                            Note:- 1. Default value --> 0.01\n",
    "                                   2. used in \"leaky_relu\" function defined later\n",
    "                  \n",
    "                  grad_check --> Boolean\n",
    "                           Note:- 1. if set to True, checks if Back_Prop is implemented correctly\n",
    "                                  2. Default --> False\n",
    "\n",
    "                  model_type --> type --> str\n",
    "                            Note:- 1. defines the problem on which this class is used \n",
    "                                   2. Default value --> \"binary\" for binary classification problem\n",
    "                                   3. other values --> \"multi\" for multi-class classification problem\n",
    "                                                       \"reg\" for regression problem   \n",
    "\n",
    "                  params --> dictionary to keep track of Weights and bais of the model.\n",
    "                            Note:- 1. batch_norm = True, also stored gama_norm and beta_norm\n",
    "\n",
    "                  linear_model --> dictionary to keep track of Z's of forward propogation.\n",
    "                            Note:- 1. Z = W*A + b (genral form)\n",
    "\n",
    "                  activation_model --> dictionary to keep track of A's of forward propogation.\n",
    "                            Note:- 1. A = activation_function(Z)\n",
    "\n",
    "                  grads --> dictionary to keep track of derivatives computed in backward propogation.\n",
    "                            Note:- 1. dA's, dZ's, dW's and db's.\n",
    "\n",
    "                    \n",
    "        '''\n",
    "        \n",
    "        self.l_dims = l_dims\n",
    "        self.activation = activation\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_interval = decay_interval\n",
    "        self.is_decay = is_decay\n",
    "        self.epsilon = epsilon\n",
    "        self.lambd = lambd\n",
    "        self.Dropout = Dropout\n",
    "        if keep_prob is None:\n",
    "            keep_prob = []\n",
    "        self.keep_prob = keep_prob\n",
    "        self.batch_norm = batch_norm\n",
    "        self.cost_reg = cost_reg\n",
    "        self.learning_algo = learning_algo\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.leaky_para = leaky_para\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        self.params = {}  \n",
    "        self.linear_model = {}\n",
    "        self.activation_model = {}\n",
    "        self.grads = {}\n",
    "    \n",
    "    \n",
    "    # helper function to calculate activation function values for each layer\n",
    "    # relu, leaky_relu and tanh activation function are used in hidden layers\n",
    "    # and they are not used in output layer.\n",
    "    # model --> \"binary\" --> output layer = \"sigmoid\"\n",
    "    # model --> \"multi\"  --> output layer = \"softmax\"\n",
    "    # model --> \"reg\"    --> output layer = \"linear\"\n",
    "    \n",
    "    # 1. relu function\n",
    "    def relu(self, z):\n",
    "        \n",
    "        # it return the maximum of 0 and z\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    \n",
    "    # 2. leaky_ReLU function\n",
    "    def leaky_relu(self, z):\n",
    "        \n",
    "        # return z if z >0\n",
    "        #       else returns leaky_para*z \n",
    "        # where leaky_para is user defined and can take a default value of 0.01\n",
    "        \n",
    "        return np.maximum(self.leaky_para*z, z)\n",
    "    \n",
    "    \n",
    "    # 3. sigmoid function \n",
    "    def sigmoid(self, z):\n",
    "        \n",
    "        # sigmoid(z) = 1/(1+e(-z))\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    \n",
    "    # 4. softmax function \n",
    "    def softmax(self, z):\n",
    "        \n",
    "        # softmax(z) = e^z(i)/sum(e^z(i)) \n",
    "        # sum is taken along column\n",
    "        \n",
    "        t = np.exp(z)\n",
    "        sum_val = np.sum(t, axis = 0)\n",
    "        \n",
    "        return t/sum_val\n",
    "    \n",
    "    \n",
    "    # 5. linear function (output layer activation function for regression)\n",
    "    def linear(self, z):\n",
    "        \n",
    "        # linear_activation(z) = z\n",
    "        return z\n",
    "    \n",
    "    \n",
    "    # gradient of activation functions\n",
    "    def grad_activation(self, z, activation):\n",
    "        \n",
    "        # given the type of activation function computed its derivative\n",
    "        if activation == 'relu':\n",
    "            \n",
    "            # relu = max(0, z) so for z>0, derivative = 1, else derivative = 0.\n",
    "            z[z>0.0] = 1\n",
    "            z[z<=0.0] = 0\n",
    "            \n",
    "            return z\n",
    "        \n",
    "        elif activation == \"sigmoid\":\n",
    "            \n",
    "            # sigmoid(z) = 1/(1+exp(-x))\n",
    "            # dreivative of sigmoid(z) = sigmoid(z)*(1-sigmoid(z))\n",
    "            \n",
    "            return self.sigmoid(z)*(1.0 - self.sigmoid(z))\n",
    "        \n",
    "        elif activation == \"tanh\":\n",
    "            \n",
    "            # tanh(z) = (exp(z) - exp(-z))/(exp(z)+exp(-z))\n",
    "            # derivative of tanh(z) :- (1 - tanh(z)^2)\n",
    "            \n",
    "            return 1.0 - np.power(np.tanh(z), 2)\n",
    "        \n",
    "        elif activation == \"linear\":\n",
    "            \n",
    "            # g(z) = z\n",
    "            # derivative of g(z) with respect to z = 1\n",
    "            \n",
    "            return 1\n",
    "        \n",
    "        else: # for leaky_relu activation function\n",
    "            \n",
    "            # leaky_relu = max(leaky_para*z, z), so for z>0 derivative = 1, else, it is = leaky_para\n",
    "            \n",
    "            z[z>0.0] = 1\n",
    "            z[z<=0.0] = self.leaky_para\n",
    "            \n",
    "            return z\n",
    "            \n",
    "    \n",
    "    # helper function for gradient checking\n",
    "    # parameter to vector conversion --> used in grad_checking\n",
    "    def params_to_vector(self):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                        Weights and bais are needed to convert them into column vector\n",
    "                        l_dims --> needed to iterate over each layer\n",
    "                        \n",
    "                 Returns --> an array of column vector where each W[l] and b[l] are stacked vertically\n",
    "         '''\n",
    "        \n",
    "        W_vector = self.params[\"W\" + str(1)].reshape((-1, 1))\n",
    "        b_vector = self.params[\"b\" + str(1)].reshape((-1, 1))\n",
    "        params_vector = np.concatenate(( W_vector, b_vector), axis = 0)\n",
    "        \n",
    "        # iterate over each layer\n",
    "        for l in range(2, len(self.l_dims)):\n",
    "            \n",
    "            W_vector = self.params[\"W\" + str(l)].reshape((-1, 1))\n",
    "            b_vector = self.params[\"b\" + str(l)].reshape((-1, 1))\n",
    "            params_vector = np.concatenate((params_vector, W_vector, b_vector), axis = 0)\n",
    "        \n",
    "        return params_vector\n",
    "    \n",
    "    \n",
    "    # gradients to vector conversion --> used in grad_checking\n",
    "    def grads_to_vector(self):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                        dW's and db's are needed to convert them into column vector\n",
    "                        l_dims --> needed to iterate over each layer\n",
    "                        \n",
    "                 Returns --> an array of column vector where each dW[l] and db[l] are stacked vertically\n",
    "        '''\n",
    "        \n",
    "        dW_vector = self.grads[\"dW\" + str(1)].reshape((-1, 1))\n",
    "        db_vector = self.grads[\"db\" + str(1)].reshape((-1, 1))\n",
    "        grads_vector = np.concatenate((dW_vector, db_vector), axis = 0)\n",
    "        \n",
    "        # iterate over each layer\n",
    "        for l in range(2, len(self.l_dims)):\n",
    "            \n",
    "            dW_vector = self.grads[\"dW\" + str(l)].reshape((-1, 1))\n",
    "            db_vector = self.grads[\"db\" + str(l)].reshape((-1, 1))\n",
    "            grads_vector = np.concatenate((grads_vector, dW_vector, db_vector), axis = 0)\n",
    "        \n",
    "        return grads_vector\n",
    "        \n",
    "    \n",
    "    # converting column vector of params to their original shape\n",
    "    def vector_to_params(self, vector):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                       --> l_dims, params --> to store the vectors into their respective place in params\n",
    "                       --> vector:- vector that needed to be converted \n",
    "                       \n",
    "        '''\n",
    "        \n",
    "        layer_dims = self.l_dims                # no. of hidden units of each layer including input layer\n",
    "        i = 0                                   # used in indexing and slicing of the vector \n",
    "        for l in range(1, len(layer_dims)):\n",
    "            \n",
    "            row = layer_dims[l]                 \n",
    "            column = layer_dims[l-1]\n",
    "            self.params[\"W_check\" + str(l)] = vector[i: i + row*column].reshape((row, column))\n",
    "            i += row*column\n",
    "            self.params[\"b_check\" + str(l)] = vector[i:i + row].reshape((row, 1))\n",
    "            i += row\n",
    "            \n",
    "#             print(self.params[\"W\" + str(l)].shape, \" = \", self.params[\"W_check\" + str(l)], \"\\n\")\n",
    "#             print(self.params[\"b\" + str(l)].shape, \" = \", self.params[\"b_check\" + str(l)], \"\\n\")\n",
    "            \n",
    "    \n",
    "    # grad_checking implemented\n",
    "    def grad_checking(self, X, Y):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                        ForWard_Prop(), Cost_dAL_and_dZL() --> to calculate J_plus and J_minus\n",
    "                        grads --> to get the column vector --> dJ/dtheta --> stored in grads\n",
    "                        params --> to get theta+ and theta-\n",
    "                        \n",
    "                Return :- \"confermation messgae on Back_Prop() implementation\"\n",
    "    \n",
    "        '''\n",
    "         \n",
    "        para_vector = self.params_to_vector()\n",
    "        dJ_dtheta = self.grads_to_vector()\n",
    "        num_para_values = para_vector.shape[0]\n",
    "        J_plus = np.zeros((num_para_values, 1))\n",
    "        J_minus = np.zeros((num_para_values, 1))\n",
    "        grad_approx = np.zeros((num_para_values, 1))\n",
    "        epsilon = self.epsilon                             # epsilon --> 1e-8\n",
    "        \n",
    "#         print(\"para_vector = \", para_vector, \"\\n para_vector shape = \", para_vector, \"\\n\")\n",
    "#         print(\"dJ_dtheta value = \", dJ_dtheta, \"\\n dJ_dtheta shape = \", dJ_dtheta.shape, \"\\n\")\n",
    "#         print(\"num_para_values = \", num_para_values, \"\\n\")\n",
    "#         print(\"J_plus = \", J_plus, \"\\n\")\n",
    "#         print(\"J_minus = \", J_minus, \"\\n\")\n",
    "#         print(\"grad_approx = \", grad_approx, \"\\n\")\n",
    "        \n",
    "        for i in range(num_para_values):\n",
    "            \n",
    "#             print(\"nudgeing \" + str(i) + \"th parameter = \", i, \"\\n\")\n",
    "            \n",
    "            # slight nudge to parameter theta_plus\n",
    "            theta_plus = np.copy(para_vector)\n",
    "#             print(\"before nudgeing theta_plus = \", theta_plus, \"\\n\")\n",
    "            theta_plus[i] = theta_plus[i] + epsilon\n",
    "            self.vector_to_params(theta_plus)\n",
    "            self.Forward_Prop(X, grad_check = True)\n",
    "            J_plus[i] = self.Cost_dAL_and_dZL(Y, grad_check = True)\n",
    "            \n",
    "#             print(\"after nudgeing theta_plus = \", theta_plus, \"\\n\")\n",
    "#             print(\"J_plus after nudgeing = \", J_plus, \"\\n\")\n",
    "            \n",
    "            # slight nudge to parameter theta_minus\n",
    "            theta_minus = np.copy(para_vector)\n",
    "#             print(\"before nudgeing theta_minus = \", theta_minus, \"\\n\")\n",
    "            theta_minus[i] = theta_minus[i] - epsilon\n",
    "            self.vector_to_params(theta_minus)\n",
    "            self.Forward_Prop(X, grad_check = True)\n",
    "            J_minus[i] = self.Cost_dAL_and_dZL(Y, grad_check = True)\n",
    "            \n",
    "#             print(\"after nudgeing theta_minus = \", theta_minus, \"\\n\")\n",
    "#             print(\"J_minus after nudgeing = \", J_minus, \"\\n\")\n",
    "            \n",
    "            # grad_approx calculation\n",
    "            grad_approx[i] = (J_plus[i] - J_minus[i])/(2*epsilon)\n",
    "            \n",
    "#             print(\"grad_approx after nudgeing = \", grad_approx, \"\\n\")\n",
    "            \n",
    "        numerator = np.linalg.norm(dJ_dtheta - grad_approx)\n",
    "        denominator = np.linalg.norm(dJ_dtheta) + np.linalg.norm(grad_approx)\n",
    "        difference = numerator/denominator\n",
    "        \n",
    "#         print(\"final difference = \", difference, \"\\n\")\n",
    "        \n",
    "        if difference > 2*epsilon:\n",
    "            \n",
    "            print(\"There is a mistake in the Back_Prop! difference = \" + str(difference))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            print(\"Back_Prop works perfectly fine! difference = \" + str(difference))\n",
    "\n",
    "            \n",
    "    # random mini batches for training of the model\n",
    "    def Random_mini_batches(self, X, Y, seed):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: --> mini_batch_size to define the size of each mini batch\n",
    "                    X, Y --> to convert them into mini batches of size mini_batch_size\n",
    "                    seed --> so that each time same mini batches are created\n",
    "                    \n",
    "             Returns: mini_batches --> list containing tuples of X_batch and Y_batch\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        M = X.shape[1]                                         # total no. of training examples\n",
    "        mini_batches = []                                      # to store X_batch and Y_batch\n",
    "        np.random.seed(seed)                                   # to always get the same random shuffling of X and Y\n",
    "        m = self.mini_batch_size                               # size of mini_batch\n",
    "        \n",
    "        # Shuffle X and Y randomly\n",
    "        permutation = list(np.random.permutation(M))           # gives a list of shuffled list\n",
    "        shuffled_X  = X[:, permutation]\n",
    "        \n",
    "        if self.model_type == \"multi\":\n",
    "            shuffled_Y = Y[:, permutation].reshape(Y.shape)\n",
    "        else:\n",
    "            shuffled_Y = Y[:, permutation].reshape((1, M))\n",
    "            \n",
    "        num_full_batches = math.floor(M/m)                     # no. of mini_batches of size mini_batch_size\n",
    "        \n",
    "#         print(\"total no of training examples = \", M, \"\\n\")\n",
    "#         print(\"size of mini_batch defined = \", m, \"\\n\")\n",
    "#         print(\"model_type = \", self.model_type, \"\\n\")\n",
    "#         print(\"shuffled_X = \", shuffled_X, \"\\n\")\n",
    "#         print(\"shuffled_Y = \", shuffled_Y, \"\\n\")\n",
    "#         print(\"total no of num_full_batches = \", num_full_batches, \" with  size = \", m, \"\\n\")\n",
    "        \n",
    "        for i in range(num_full_batches):\n",
    "            \n",
    "            mini_batch_X = shuffled_X[:, i*m: (i+1)*m]\n",
    "            mini_batch_Y = shuffled_Y[:, i*m: (i+1)*m]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "        \n",
    "        if M % self.mini_batch_size != 0:\n",
    "            \n",
    "            mini_batch_X = shuffled_X[:, num_full_batches*m : M]\n",
    "            mini_batch_Y = shuffled_Y[:, num_full_batches*m : M]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "            \n",
    "        return mini_batches\n",
    "    \n",
    "    \n",
    "    # learning_rate decay on fixed interval\n",
    "    def decay(self, epoch):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                         learning_rate, decay_rate, decay_interval, epoch needed to calculate new learning rate\n",
    "                         \n",
    "           Returns :- New learning rate\n",
    "        '''\n",
    "        \n",
    "        old_rate = self.learning_rate\n",
    "        decay_rate = self.decay_rate\n",
    "        interval = self.decay_interval\n",
    "        if epoch % interval == 0:\n",
    "            \n",
    "#             print(\"old learning rate = \", old_rate, \"\\n\")\n",
    "            self.learning_rate = old_rate/(1 + decay_rate*(epoch/interval))\n",
    "#             print(\"new learning rate = \", self.learning_rate, \"\\n\")\n",
    "     \n",
    "    # Updating mean_test and var_test using exponentially weighted moving average\n",
    "    def Update_mean_and_var(self):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                        beta1, mean_test[l], mean_var[l], l_dims --. to update mean_test and mean_var\n",
    "                        --> updating using exponentially weighted moving average\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        layer_dims = self.l_dims\n",
    "        beta1 = self.beta1                                   # beta1 used in exponentially weighted moving average\n",
    "            \n",
    "        for l in range(1, len(layer_dims)):\n",
    "            \n",
    "            # parameter used for updating mean and variance using exponentially weighted moving average\n",
    "            old_mean_test = self.params[\"mean_test\" + str(l)]    # old mean_test calculated till previous mini_batch                      \n",
    "            mean_batch = self.params[\"mean\" + str(l)]            # mean calculated for perticular mini_batch for layer l\n",
    "            old_var_test = self.params[\"var_test\" + str(l)]      # old var_test calculated till previous mini_batch                      \n",
    "            var_batch = self.params[\"var\" + str(l)]              # variance calculated for perticular mini_batch for layer l\n",
    "            \n",
    "            # applying the formula and updating the values\n",
    "            self.params[\"mean_test\" + str(l)] = beta1*old_mean_test + (1-beta1)*mean_batch\n",
    "            self.params[\"var_test\" + str(l)] = beta1*old_var_test + (1-beta1)*var_batch\n",
    "            \n",
    "#             print(\"beta1 = \", beta1, \"\\n\")\n",
    "#             print(\"old mean_test\" + str(l), \" = \", old_mean_test, \"\\n\")\n",
    "#             print(\"mean\" + str(l), \"_batch = \", mean_batch, \"\\n\")\n",
    "#             print(\"new mean_test\" + str(l), \" = \", self.params[\"mean_test\" + str(l)], \"\\n\")\n",
    "#             print(\"old var_test\" + str(l), \" = \", old_var_test, \"\\n\")\n",
    "#             print(\"var\" + str(l), \"_batch = \", var_batch, \"\\n\")\n",
    "#             print(\"new var_test\" + str(l), \" = \", self.params[\"var_test\" + str(l)], \"\\n\")\n",
    "            \n",
    "            \n",
    "    # parameter initlization\n",
    "    def Parameter_Initialization(self):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                        l_dims --> To iterate over each layer and access the # hidden unit \n",
    "                                     \n",
    "                        params --> To initialize and store parameters\n",
    "                                  1. W, b, gama_norm, beta_norm, V_dW, V_db, S_dW and S_db \n",
    "                                    \n",
    "                        activation --> To get the factor for each layer, activation of that layer is needed\n",
    "                                          \n",
    "        '''\n",
    "#         print(\"Running Parameter Initialization function ! \\n\")\n",
    "#         print(\"batch_norm = \", self.batch_norm, \"\\n\")\n",
    "#         print(\"learning Algorithm = \", self.learning_algo, \"\\n\")\n",
    "        \n",
    "        activation_fun = self.activation        # activation function list for the layers\n",
    "        layer_dims = self.l_dims                # list containing # hidden unit for each layer including input layer \n",
    "        \n",
    "        for l in range(1, len(layer_dims)):\n",
    "            \n",
    "            # \"he\" initilization\n",
    "            if activation_fun[l-1] == \"relu\" or activation_fun[l-1] == \"leaky_relu\":     \n",
    "                \n",
    "                mul_factor = np.sqrt(2/layer_dims[l-1])\n",
    "                \n",
    "                \n",
    "            # \"xavier\" initilization for \"tanh\" and \"sigmoid\" activation function\n",
    "            else:\n",
    "                \n",
    "                mul_factor = np.sqrt(1/layer_dims[l-1])\n",
    "               \n",
    "            self.params[\"W\"+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*mul_factor     \n",
    "            self.params[\"b\"+str(l)] = np.zeros((layer_dims[l], 1))                                   \n",
    "            \n",
    "                \n",
    "#             print(\"mul_factor = \", mul_factor, \"\\n\")\n",
    "#             print(\"W\" + str(l), \" = \", self.params[\"W\" + str(l)], \"\\n\")\n",
    "#             print(\"b\" + str(l), \" = \", self.params[\"b\" + str(l)], \"\\n\")\n",
    "                \n",
    "            # batch_norm\n",
    "            if self.batch_norm == True:\n",
    "                \n",
    "                # mean_test[l] is exponentially weighted avg values computed over all mini_batch_X\n",
    "                # var_test[l] is exponentially weighted avg values computed over all mini_batch_X\n",
    "                # mean[l] stores the mean of the current mini_batch_X\n",
    "                # var[l] stores the variance of the current minin_batch_X\n",
    "                \n",
    "                shape_norm = (layer_dims[l], 1)\n",
    "                self.params[\"gama\" + str(l)] = np.random.randn(shape_norm[0], shape_norm[1])*mul_factor\n",
    "                self.params[\"beta\" + str(l)] = np.zeros((shape_norm))\n",
    "                self.params[\"mean_test\" + str(l)] = np.zeros((shape_norm)) \n",
    "                self.params[\"var_test\" + str(l)] = np.zeros((shape_norm)) \n",
    "                self.params[\"mean\" + str(l)] = np.zeros((shape_norm)) \n",
    "                self.params[\"var\" + str(l)] = np.zeros((shape_norm)) \n",
    "\n",
    "#                 print(\"shape for norm = \", shape_norm, \"\\n\")\n",
    "#                 print(\"gama\" + str(l), \" = \", self.params[\"gama\" + str(l)], \"\\n\")\n",
    "                \n",
    "            \n",
    "            # \"momentum\" learning algorithm or \"Adam\" learning algorithm\n",
    "            if self.learning_algo == \"momentum\" or self.learning_algo == \"Adam\":\n",
    "                \n",
    "                self.params[\"V_dW\"+str(l)] = np.zeros((layer_dims[l], layer_dims[l-1]))\n",
    "                self.params[\"V_db\"+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "                \n",
    "#                 print(\"V_dW\" + str(l), \" = \", self.params[\"V_dW\" + str(l)], \"\\n\")\n",
    "#                 print(\"V_db\" + str(l), \" = \", self.params[\"V_db\" + str(l)], \"\\n\")\n",
    "                \n",
    "                # for batch_norm\n",
    "                if self.batch_norm == True:\n",
    "                    \n",
    "                    self.params[\"V_dgama\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "                    self.params[\"V_dbeta\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "                    \n",
    "#                     print(\"V_dgama\" + str(l), \" = \", self.params[\"V_dgama\" + str(l)], \"\\n\")\n",
    "#                     print(\"V_dbeta\" + str(l), \" = \", self.params[\"V_dbeta\" + str(l)], \"\\n\")\n",
    "                \n",
    "\n",
    "            # \"RMSprop\" learning algorithm or \"Adam\" learning algorithm\n",
    "            if self.learning_algo == \"RMSprop\" or self.learning_algo == \"Adam\":\n",
    "                \n",
    "                self.params[\"S_dW\"+str(l)] = np.zeros((layer_dims[l], layer_dims[l-1]))\n",
    "                self.params[\"S_db\"+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "                \n",
    "#                 print(\"S_dW\" + str(l), \" = \", self.params[\"S_dW\" + str(l)], \"\\n\")\n",
    "#                 print(\"S_db\" + str(l), \" = \", self.params[\"S_db\" + str(l)], \"\\n\")\n",
    "                \n",
    "                \n",
    "                # for batch_norm\n",
    "                if self.batch_norm == True:\n",
    "                    \n",
    "                    self.params[\"S_dgama\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "                    self.params[\"S_dbeta\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "                    \n",
    "#                     print(\"S_dgama\" + str(l), \" = \", self.params[\"S_dgama\" + str(l)], \"\\n\")\n",
    "#                     print(\"S_dbeta\" + str(l), \" = \", self.params[\"S_dbeta\" + str(l)], \"\\n\")\n",
    "                \n",
    "\n",
    "                    \n",
    "    # forward propogation\n",
    "    def Forward_Prop(self, X, grad_check = False, function = \"fit\"):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                        l_dims --> used just to iterate over the layers \n",
    "                        params --> used to calculate Z \n",
    "                        linear_model --> to keep track of Z\n",
    "                        activation --> to get the type of activation function used at each layer\n",
    "                        activation_model --> to keep track of A\n",
    "                    \n",
    "                    X --> training set\n",
    "                        Note: stored in activation_model as \"A0\"\n",
    "                              shape of X = (l_dims[0], batch_size)\n",
    "                              l_dims[0] = n_features of X\n",
    "                              \n",
    "        '''\n",
    "\n",
    "        self.activation_model[\"A0\"] = X \n",
    "        layer_dims = self.l_dims\n",
    "        epsilon = self.epsilon                            # epsilon = 1e-8\n",
    "        activation_fun = self.activation\n",
    "        \n",
    "#         print(\"check type = \", grad_check, \"\\n\")\n",
    "#         print(\"batch_norm = \", self.batch_norm, \"\\n\")\n",
    "#         print(\"Dropout = \", self.Dropout, \"\\n\")\n",
    "#         print(\"Used for = \", function, \"\\n\")\n",
    "        \n",
    "        for l in range(1, len(layer_dims)): \n",
    "            \n",
    "            # if gradient_checking is set to True\n",
    "            if grad_check == True:\n",
    "                \n",
    "                W = self.params[\"W_check\" + str(l)]\n",
    "                b = self.params[\"b_check\" + str(l)]\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                W = self.params[\"W\" + str(l)]\n",
    "                b = self.params[\"b\" + str(l)]\n",
    "            \n",
    "            A_prev = self.activation_model[\"A\"+str(l-1)]\n",
    "            \n",
    "#             print(\"W\" + str(l), \" = \", W, \"\\n\")\n",
    "#             print(\"b\" + str(l), \" = \", b, \"\\n\")\n",
    "#             print(\"A_prev or A\"+str(l-1), \" = \",A_prev, \"\\n\")\n",
    "            \n",
    "            # batch_normalization on Z[l] if batch_norm = True\n",
    "            if self.batch_norm:\n",
    "                \n",
    "                Z_orig = np.dot(W, A_prev) + b\n",
    "                self.linear_model[\"Z_orig\" + str(l)] = Z_orig\n",
    "#                 print(\"Z_orig = \", Z_orig, \"\\n\")\n",
    "                \n",
    "                # During fitting the model\n",
    "                if function == \"fit\":\n",
    "                    \n",
    "                    mean = np.mean(Z_orig, axis = 1, keepdims= True) # mean is caluclated over each hidden unit (along row)\n",
    "                    self.params[\"mean\"+str(l)] = mean\n",
    "                    var = np.var(Z_orig, axis = 1, keepdims= True)   # variance is calculated over each hidden unit (along row)\n",
    "                    self.params[\"var\" + str(l)] = var\n",
    "                    \n",
    "#                     print(\"mean for Z_orig\"+ str(l), \" = \", mean, \"\\n\", \"shape of mean = \", mean.shape, \"\\n\")\n",
    "#                     print(\"var for Z_orig\"+ str(l), \" = \", var , \"\\n\", \"shape of var = \", var.shape, \"\\n\")\n",
    "                \n",
    "                \n",
    "                # During predicting for the model\n",
    "                else:\n",
    "                    \n",
    "                    mean = self.params[\"mean_test\" + str(l)]\n",
    "                    var = self.params[\"var_test\" + str(l)]\n",
    "                    \n",
    "#                     print(\"mean used during testing = \", mean, \"\\n\", \"shape of mean = \", mean.shape, \"\\n\")\n",
    "#                     print(\"var used during testing = \", var , \"\\n\", \"shape of var = \", var.shape, \"\\n\")\n",
    "                \n",
    "                \n",
    "                Z_norm = (Z_orig - mean)/(np.sqrt(var + epsilon))\n",
    "                self.linear_model[\"Z_norm\" + str(l)] = Z_norm\n",
    "                gama = self.params[\"gama\" + str(l)]\n",
    "                beta = self.params[\"beta\" + str(l)]\n",
    "                Z = np.multiply(gama, Z_norm) + beta\n",
    "                self.linear_model[\"Z\"+str(l)] = Z\n",
    "                \n",
    "#                 print(\"Z_norm\"+str(l), \" = \", Z_norm, \"\\n\")\n",
    "#                 print(\"gama\"+str(l), \" = \", gama, \"\\n\")\n",
    "#                 print(\"beta\"+str(l), \" = \", beta, \"\\n\")\n",
    "            \n",
    "            # batch_norm = False\n",
    "            else:\n",
    "                \n",
    "                Z = np.dot(W, A_prev) + b\n",
    "                self.linear_model[\"Z\"+str(l)] = Z\n",
    "                \n",
    "#             print(\"Z\"+str(l), \" = \", Z, \"\\n\")\n",
    "#             print(\"activation function of\" + str(l) + \"_th layer = \", activation_fun[l-1], \"\\n\")\n",
    "            # applying activation function of Z[l]\n",
    "            \n",
    "            if activation_fun[l-1] == 'relu':\n",
    "                \n",
    "                A = self.relu(Z)                   # post_activation_function value\n",
    "                \n",
    "            elif activation_fun[l-1] == 'sigmoid':\n",
    "                \n",
    "                A = self.sigmoid(Z)\n",
    "                \n",
    "            elif activation_fun[l-1] == 'tanh' :\n",
    "                \n",
    "                A = np.tanh(Z)\n",
    "                \n",
    "            elif activation_fun[l-1] == \"softmax\":\n",
    "                \n",
    "                A = self.softmax(Z)\n",
    "                \n",
    "            elif activation_fun[l-1] == \"linear\":\n",
    "                \n",
    "                A = self.linear(Z)\n",
    "                \n",
    "            else:  # activation = 'leaky_relu'\n",
    "                \n",
    "                A = self.leaky_relu(Z)\n",
    "            \n",
    "            self.activation_model[\"A\" + str(l)] = A\n",
    "            # Applying Dropout (only applied during fitting and not in predicting)\n",
    "            if self.Dropout and function == \"fit\":  \n",
    "                \n",
    "                np.random.seed(0)\n",
    "                shape_D = A.shape\n",
    "                D = np.random.rand(shape_D[0], shape_D[1])\n",
    "                self.params[\"D\" + str(l)] = (D < self.keep_prob[l-1]).astype(int)\n",
    "                A = np.multiply(A, self.params[\"D\" + str(l)])\n",
    "                self.activation_model[\"A\" + str(l)] = A/self.keep_prob[l-1]    # scale up\n",
    "                \n",
    "#                 print(\"D\"+str(l), \" = \", self.params[\"D\"+str(l)], \"\\n\")\n",
    "                \n",
    "#             print(\"final A\"+str(l), \" = \", self.activation_model[\"A\"+str(l)], \"\\n\")    \n",
    "\n",
    "    # cost of the model and dAL            \n",
    "    def Cost_dAL_and_dZL(self, Y, grad_check = False):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Parameters: self:\n",
    "                        activation_model --> to get the output layer A (AL)\n",
    "                        grads --> to store dAL (derivative of cost function w.r.t AL)\n",
    "                        l_dims --> to access the AL\n",
    "                        mini_batch_size --> used for calculating cost\n",
    "                        cost_reg --> for \"reg\" model, which type of cost function is used\n",
    "                        \n",
    "                    Y --> used in calculating cost function\n",
    "                               \n",
    "        Returns:  cost function value for a given cost_type\n",
    "        '''\n",
    "#         print(\"check_type = \", grad_check, \"\\n\")\n",
    "#         print(\"model_type = \", self.model_type, \"\\n\")\n",
    "#         print(\"cost_reg = \", self.cost_reg, '\\n')\n",
    "        \n",
    "        layer_dims = self.l_dims                    \n",
    "        m = Y.shape[1]                                  # size of the current mini_batch / training set\n",
    "        L = len(layer_dims) - 1                         # final layer index\n",
    "        AL = self.activation_model[\"A\"+str(L)]          # post activation value of output layer\n",
    "        lambd = self.lambd                              # lambda --> used in calculating regularization cost\n",
    "        frobenius_norm_square = 0                       # to get the sum of W[l]^2 over each layer\n",
    "        \n",
    "#         print(\"size of batch = \", m, \"\\n\")\n",
    "        for l in range(1, len(layer_dims)):\n",
    "            \n",
    "            # if gradient_checking is set to True\n",
    "            if grad_check == True:\n",
    "                \n",
    "                W = self.params[\"W_check\" + str(l)]\n",
    "                b = self.params[\"b_check\" + str(l)]\n",
    "                \n",
    "#                 print(\"W_check\"+str(l), \" = \", W, \"\\n\")\n",
    "#                 print(\"b_check\"+str(l), \" = \", b, \"\\n\")\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                W = self.params[\"W\" + str(l)]\n",
    "                b = self.params[\"b\" + str(l)]\n",
    "                \n",
    "#                 print(\"W\"+str(l), \" = \", W, \"\\n\")\n",
    "#                 print(\"b\"+str(l), \" = \", b, \"\\n\")\n",
    "            \n",
    "            frobenius_norm_square += np.power(np.linalg.norm(W),2)\n",
    "            \n",
    "#             print(\"frobenius_norm_square = \", frobenius_norm_square, \"\\n\")\n",
    "            \n",
    "#         print(\"frobenius_norm_square final value = \", frobenius_norm_square, \"\\n\")\n",
    "#         print(\"A\"+str(L), \"= \", AL, \"\\n\")\n",
    "#         print(\"Y = \", Y, \"\\n\")\n",
    "        \n",
    "        Regularization_cost = lambd*frobenius_norm_square/(2*m)\n",
    "#         print(\"Regularization_cost = \", Regularization_cost, \"\\n\")\n",
    "        \n",
    "        # for binary classification\n",
    "        if self.model_type == \"binary\": \n",
    "            \n",
    "            # for \"binary\" model, only one hidden unit is used at output layer\n",
    "            # shape of AL --> (1, m), shape of Y --> (1, m) (Y is given as user input)\n",
    "            \n",
    "            cost = (-1/m)*(np.dot(Y, np.log(AL).T)+np.dot(1-Y, np.log(1-AL).T)) + Regularization_cost\n",
    "            self.grads[\"dA\"+str(L)] = -1*np.divide(Y, AL) + np.divide(1-Y, 1-AL)\n",
    "            cost = np.squeeze(cost)                     # if by chance cost is not a value and an array, np.squeeze is used. \n",
    "            self.grads[\"dZ\" + str(L)] = AL - Y          # because activation function at output layer will always be sigmoid\n",
    "            \n",
    "        # for multi-class classification\n",
    "        elif self.model_type == \"multi\":\n",
    "            \n",
    "            cost = (-1/m)*np.sum(np.multiply(Y, np.log(AL))) + Regularization_cost\n",
    "            self.grads[\"dA\"+str(L)] = -Y/AL\n",
    "            cost = np.squeeze(cost)\n",
    "            self.grads[\"dZ\" + str(L)] = AL - Y\n",
    "            \n",
    "        \n",
    "        # for regression \n",
    "        else:\n",
    "            \n",
    "            # for mean squared error\n",
    "            if self.cost_reg == \"MSE\":\n",
    "\n",
    "                cost = (1/m)*np.sum((Y-AL)*(Y-AL)) + Regularization_cost\n",
    "                self.grads[\"dA\"+str(L)] = -2*(Y-AL)\n",
    "                cost = np.squeeze(cost)\n",
    "                \n",
    "            # for mean absolute error\n",
    "            else:\n",
    "\n",
    "                cost = (1/m)*np.sum(np.abs(Y-AL)) + regularization_cost\n",
    "                self.grads[\"dA\"+str(L)] = (AL-Y)/(np.abs(Y-AL))\n",
    "                cost = np.squeeze(cost)\n",
    "                \n",
    "            self.grads[\"dZ\" + str(L)] = self.grads[\"dA\" + str(L)]        # activation function is linear\n",
    "            \n",
    "#         print(\"cost = \", cost, \"\\n\")\n",
    "#         print(\"dA\"+str(L), \" = \", self.grads[\"dA\"+str(L)], \"\\n\")\n",
    "#         print(\"dZ\"+str(L), \" = \", self.grads[\"dZ\"+str(L)], \"\\n\")\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    # back propogation\n",
    "    def Back_Prop(self):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                        batch_size --> used in calculating grads\n",
    "                        l_dims --> to  iterate over each layer\n",
    "                        grads --> to store derivatives \n",
    "                        linear_model, activation_model, params --> used to calculate grads\n",
    "                        var --> stored variance is used in calculation\n",
    "                                 \n",
    "        '''\n",
    "        \n",
    "        layer_dims = self.l_dims\n",
    "        L = len(layer_dims) - 1                  # output layer number\n",
    "        m = self.grads[\"dA\"+str(L)].shape[1]     # size of current mini batch\n",
    "        I_m = np.ones((1, m))\n",
    "        lambd = self.lambd\n",
    "        epsilon = self.epsilon\n",
    "        activation_fun = self.activation\n",
    "        \n",
    "#         print(\"size of batch = \", m, \"\\n\")\n",
    "#         print(\"I_m = \", I_m, \"\\n\")\n",
    "#         print(\"batch_norm = \", self.batch_norm, \"\\n\")\n",
    "#         print(\"Dropout = \", self.Dropout, \"\\n\")\n",
    "        \n",
    "        # calculated \"dAL\" and \"dZL\" during calculating cost for the model\n",
    "        # d_Reg_W is derivative of regularization cost w.r.t W[l]\n",
    "        \n",
    "        for l in reversed(range(1, len(layer_dims))):\n",
    "            \n",
    "            # will be used in calculation of grads\n",
    "            W = self.params[\"W\" + str(l)]                     \n",
    "            d_Reg_W = lambd*W/m\n",
    "            dZ = self.grads[\"dZ\" + str(l)]\n",
    "            A_prev = self.activation_model[\"A\" + str(l-1)]\n",
    "            \n",
    "            # when batch norm is applied to Z[l]\n",
    "            if self.batch_norm == True:\n",
    "                \n",
    "                # calculating d_gama[l], d_beta[l], dZ_orig[l], dW[l], db[l], dA[l-1], dZ[l-1]\n",
    "                # parameter from norm used in calculations.\n",
    "                \n",
    "                Z_norm = self.linear_model[\"Z_norm\" + str(l)]\n",
    "                var = self.params[\"var\" + str(l)]\n",
    "                gama = self.params[\"gama\" + str(l)]\n",
    "                \n",
    "                # forumlas applied to calculate d_gama[l], d_beta[l], dZ_orig[l], dW[l], db[l], dA[l-1], dZ[l-1]\n",
    "                \n",
    "                d_gama = np.sum(np.multiply(dZ, Z_norm), axis = 1, keepdims = True)/m\n",
    "                d_beta = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "                variable1 = m*dZ\n",
    "                variable2 = np.dot(d_beta, I_m)\n",
    "                variable3 = np.multiply(d_gama, Z_norm)\n",
    "                variable4 = m*np.sqrt(var + epsilon)\n",
    "                dZ_orig = np.multiply(gama, (variable1 - variable2 - variable3))/variable4\n",
    "                dW = (1/m)*np.dot(dZ_orig, A_prev.T) + d_Reg_W\n",
    "                db = (1/m)*np.sum(dZ_orig, axis = 1, keepdims = True)\n",
    "                dA_prev = np.dot(W.T, dZ_orig)\n",
    "                \n",
    "                # storing the values in dictionary for further use\n",
    "                self.grads[\"d_gama\" + str(l)] = d_gama\n",
    "                self.grads[\"d_beta\" + str(l)] = d_beta\n",
    "                self.grads[\"dZ_orig\" + str(l)] = dZ_orig\n",
    "                self.grads[\"dW\"+str(l)] = dW\n",
    "                self.grads[\"db\"+str(l)] = db\n",
    "                self.grads[\"dA\"+str(l-1)] = dA_prev\n",
    "                \n",
    "#                 print(\"dZ\"+str(l), \" = \", dZ, \"\\n\")\n",
    "#                 print(\"Z_norm\"+str(l), \" = \", Z_norm, \"\\n\")\n",
    "#                 print(\"d_gama\"+str(l), \" = \", d_gama, \"\\n\")\n",
    "#                 print(\"d_beta\"+str(l), \" = \", d_beta, \"\\n\")\n",
    "#                 print(\"m*dZ\"+str(l), \" = \", variable1, \"\\n\")\n",
    "#                 print(\"d_beta\"+str(l), \"*I_m = \", variable2, \"\\n\")\n",
    "#                 print(\"d_gama_norm\"+str(l),\".*Z_norm\"+str(l), \" = \", variable3, \"\\n\")\n",
    "#                 print(\"m*sqrt(var\"+str(l), \"+ epsilon) = \", variable4, \"\\n\")\n",
    "#                 print(\"gama\"+str(l), \" = \", gama, \"\\n\")\n",
    "#                 print(\"dZ_orig\"+str(l), \" = \", dZ_orig, \"\\n\")\n",
    "#                 print(\"d_Reg_W = \", d_Reg_W, \"\\n\")\n",
    "#                 print(\"A\"+str(l-1), \" = \", A_prev, \"\\n\")\n",
    "#                 print(\"dW\"+str(l), \" = \", dW, \"\\n\")\n",
    "#                 print(\"db\"+str(l), \" = \", db, \"\\n\")\n",
    "#                 print(\"dA\"+str(l-1), \" = \", dA_prev, \"\\n\")\n",
    "                \n",
    "            # When batch norm is not applied\n",
    "            else:\n",
    "                \n",
    "                # formulas used to calculate dW[l], db[l], dA[l-1]\n",
    "                dW = (1/m)*np.dot(dZ, A_prev.T) + d_Reg_W\n",
    "                db = (1/m)*np.sum(dZ, axis = 1, keepdims = True)\n",
    "                dA_prev = np.dot(W.T, dZ)\n",
    "                \n",
    "                # storing the values in grads dictionary for further use\n",
    "                self.grads[\"dW\"+str(l)] = dW\n",
    "                self.grads[\"db\"+str(l)] = db\n",
    "                self.grads[\"dA\"+str(l-1)] = dA_prev\n",
    "                \n",
    "                \n",
    "#                 print(\"d_Reg_W = \", d_Reg_W, \"\\n\")\n",
    "#                 print(\"dZ\"+str(l), \" = \", self.grads[\"dZ\"+str(l)], \"\\n\")\n",
    "#                 print(\"A\"+str(l-1), \" = \", self.activation_model[\"A\"+str(l-1)], \"\\n\")\n",
    "#                 print(\"dW\"+str(l), \" = \", self.grads[\"dW\"+str(l)], \"\\n\")\n",
    "#                 print(\"db\"+str(l), \" = \", self.grads[\"db\"+str(l)], \"\\n\")\n",
    "#                 print(\"dA\"+str(l-1), \" = \", self.grads[\"dA\"+str(l-1)], \"\\n\")\n",
    "                \n",
    "            if l != 1:\n",
    "                \n",
    "                Z_prev = self.linear_model[\"Z\" + str(l-1)]\n",
    "                grad_value = self.grad_activation(Z_prev, activation_fun[l-2])\n",
    "                dZ_prev = np.multiply(dA_prev, grad_value)\n",
    "                self.grads[\"dZ\"+str(l-1)] = dZ_prev\n",
    "                \n",
    "#                 print(\"Z_prev = \", Z_prev)\n",
    "#                 print(\"grad_value = \", grad_value, \"\\n\")\n",
    "#                 print(\"dZ\"+str(l-1), \" = \", dZ_prev, \"\\n\")\n",
    "\n",
    "            # Applying Dropout to \"dA[l]\" if possible\n",
    "            if self.Dropout:\n",
    "\n",
    "#                 print(\"dA\"+str(l), \" before = \", self.grads[\"dA\"+str(l)], \"\\n\")\n",
    "                D = self.params[\"D\" + str(l)]\n",
    "                dA = self.grads[\"dA\" + str(l)]\n",
    "                dA = np.multiply(dA, D)\n",
    "                dA = dA/self.keep_prob[l-1]\n",
    "                self.grads[\"dA\" + str(l)] = dA\n",
    "                \n",
    "#                 print(\"D\"+str(l), \" = \", D, \"\\n\")\n",
    "#                 print(\"dA\"+str(l), \" after = \", dA, \"\\n\")\n",
    "\n",
    "    # updating the parameters Wl's and bl's        \n",
    "    def Parameter_Update(self, t):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self:\n",
    "                        t --> counter for \"Adam\"\n",
    "                        learning_algo --> which algorithm to use for updating Weights and bais of the model\n",
    "                        l_dims --> to iterate over each layer\n",
    "                        params, learning_rate, grads --> to update the parameter of the model\n",
    "                        \n",
    "        '''\n",
    "        \n",
    "        layer_dims = self.l_dims\n",
    "        beta1 = self.beta1                                   # parameter used for \"momentum\" or \"Adam\" Algorithm\n",
    "        beta2 = self.beta2                                   # parameter used for \"RMSprop\" or \"Adam\" Algorithm\n",
    "        epsilon = self.epsilon\n",
    "        \n",
    "#         print(\"learning Algorithm = \", self.learning_algo, \"\\n\")\n",
    "#         print(\"batch_norm = \", self.batch_norm, \"\\n\")\n",
    "#         print(\"beta1 = \", beta1, \"\\n\")\n",
    "#         print(\"beta2 = \", beta2, \"\\n\")\n",
    "        \n",
    "        # updating the parameter using different learning algorithms\n",
    "        for l in range(1, len(layer_dims)):\n",
    "            \n",
    "            # \"gd\", standard gradient descent algorithm to update W's and b's\n",
    "            if self.learning_algo == \"gd\":\n",
    "                \n",
    "                factor_dW = self.grads[\"dW\" + str(l)]\n",
    "                factor_db = self.grads[\"db\" + str(l)]\n",
    "                \n",
    "                # for batch_norm\n",
    "                if self.batch_norm == True:\n",
    "                    \n",
    "                    factor_dgama = self.grads[\"d_gama\" + str(l)]\n",
    "                    factor_dbeta = self.grads[\"d_beta\" + str(l)] \n",
    "            \n",
    "            \n",
    "            # \"momentum\" algorithm\n",
    "            elif self.learning_algo == \"momentum\":\n",
    "                \n",
    "                # parameter used for updating V_dW and V_db\n",
    "                V_dW = self.params[\"V_dW\" + str(l)]\n",
    "                V_db = self.params[\"V_db\" + str(l)]\n",
    "                dW = self.grads[\"dW\" + str(l)]\n",
    "                db = self.grads[\"db\" + str(l)]\n",
    "                \n",
    "                # updating and storing V_dW and V_db\n",
    "                self.params[\"V_dW\" + str(l)] = beta1*V_dW + (1-beta1)*dW\n",
    "                self.params[\"V_db\" + str(l)] = beta1*V_db + (1-beta1)*db\n",
    "                \n",
    "                factor_dW = self.params[\"V_dW\" + str(l)]\n",
    "                factor_db = self.params[\"V_db\" + str(l)]\n",
    "                \n",
    "#                 print(\"V_dW\" + str(l), \" before = \", V_dW, \"\\n\")\n",
    "#                 print(\"V_db\" + str(l), \" before = \", V_db, \"\\n\")\n",
    "#                 print(\"dW\" + str(l), \" = \", dW, \"\\n\")\n",
    "#                 print(\"db\" + str(l), \" = \", db, \"\\n\")\n",
    "#                 print(\"V_dW\" + str(l), \" after = \", self.params[\"V_dW\" + str(l)], \"\\n\")\n",
    "#                 print(\"V_db\" + str(l), \" after = \", self.params[\"V_db\" + str(l)], \"\\n\")\n",
    "                \n",
    "                \n",
    "                # for batch_norm\n",
    "                if self.batch_norm == True:\n",
    "                    \n",
    "                    # parameter used for updating V_dgama and V_dbeta\n",
    "                    V_dgama = self.params[\"V_dgama\" + str(l)]\n",
    "                    V_dbeta = self.params[\"V_dbeta\" + str(l)]\n",
    "                    d_gama = self.grads[\"d_gama\" + str(l)]\n",
    "                    d_beta = self.grads[\"d_beta\" + str(l)]\n",
    "                    \n",
    "                    # updating and storing V_dgama and V_dbeta\n",
    "                    self.params[\"V_dgama\" + str(l)] = beta1*V_dgama + (1 - beta1)*d_gama\n",
    "                    self.params[\"V_dbeta\" + str(l)] = beta1*V_dbeta + (1 - beta1)*d_beta\n",
    "                    \n",
    "                    factor_dgama = self.params[\"V_dgama\" + str(l)]\n",
    "                    factor_dbeta = self.params[\"V_dbeta\" + str(l)]\n",
    "                    \n",
    "#                     print(\"V_dgama\"+str(l), \" before = \", V_dgama, \"\\n\")\n",
    "#                     print(\"V_dbeta\"+str(l), \" before = \", V_dbeta, \"\\n\")\n",
    "#                     print(\"d_gama\"+str(l), \" = \", d_gama, \"\\n\")\n",
    "#                     print(\"d_beta\"+str(l), \" = \", d_beta, \"\\n\")\n",
    "#                     print(\"V_dgama\"+str(l), \" after = \", self.params[\"V_dgama\"+str(l)], \"\\n\")\n",
    "#                     print(\"V_dbeta\"+str(l), \" after = \", self.params[\"V_dbeta\"+str(l)], \"\\n\")\n",
    "                \n",
    "                    \n",
    "            # \"RMSprop\" algorithm\n",
    "            elif self.learning_algo == \"RMSprop\":\n",
    "                \n",
    "                # parameter used for updating S_dW and S_db\n",
    "                S_dW = self.params[\"S_dW\" + str(l)]\n",
    "                S_db = self.params[\"S_db\" + str(l)]\n",
    "                dW = self.grads[\"dW\" + str(l)]\n",
    "                db = self.grads[\"db\" + str(l)]\n",
    "                \n",
    "                # updating and storing S_dW and S_db\n",
    "                self.params[\"S_dW\" + str(l)] = beta2*S_dW + (1-beta2)*np.power(dW, 2)\n",
    "                self.params[\"S_db\" + str(l)] = beta2*S_db + (1-beta2)*np.power(db, 2)\n",
    "                \n",
    "                factor_dW = dW/(np.sqrt(self.params[\"S_dW\" + str(l)]) + epsilon)\n",
    "                factor_db = db/(np.sqrt(self.params[\"S_db\" + str(l)]) + epsilon)\n",
    "                \n",
    "#                 print(\"S_dW\"+str(l), \" before = \", S_dW, \"\\n\")\n",
    "#                 print(\"S_db\"+str(l), \" before = \", S_db, \"\\n\")\n",
    "#                 print(\"dW\"+str(l), \" = \", dW, \"\\n\")\n",
    "#                 print(\"db\"+str(l), \" = \", db, \"\\n\")\n",
    "#                 print(\"S_dW\"+str(l), \" after = \", self.params[\"S_dW\"+str(l)], \"\\n\")\n",
    "#                 print(\"S_db\"+str(l), \" after = \", self.params[\"S_db\"+str(l)], \"\\n\")\n",
    "                \n",
    "                \n",
    "                # for batch_norm\n",
    "                if self.batch_norm == True:\n",
    "                    \n",
    "                    # parameter used for updating S_dgama and S_dbeta\n",
    "                    S_dgama = self.params[\"S_dgama\" + str(l)]\n",
    "                    S_dbeta = self.params[\"S_dbeta\" + str(l)]\n",
    "                    d_gama = self.grads[\"d_gama\" + str(l)]\n",
    "                    d_beta = self.grads[\"d_beta\" + str(l)]\n",
    "                    \n",
    "                    # updating and storing V_dgama and V_dbeta\n",
    "                    self.params[\"S_dgama\" + str(l)] = beta2*S_dgama + (1 - beta2)*np.power(d_gama, 2)\n",
    "                    self.params[\"S_dbeta\" + str(l)] = beta2*S_dbeta + (1 - beta2)*np.power(d_beta, 2)\n",
    "                    \n",
    "                    factor_dgama = d_gama/(np.sqrt(self.params[\"S_dgama\" + str(l)]) + epsilon)\n",
    "                    factor_dbeta = d_beta/(np.sqrt(self.params[\"S_dbeta\" + str(l)]) + epsilon)\n",
    "                \n",
    "#                     print(\"S_dgama\"+str(l), \" before = \", S_dgama, \"\\n\")\n",
    "#                     print(\"S_dbeta\"+str(l), \" before = \", S_dbeta, \"\\n\")\n",
    "#                     print(\"d_gama\"+str(l), \" = \", d_gama, \"\\n\")\n",
    "#                     print(\"d_beta\"+str(l), \" = \", d_beta, \"\\n\")\n",
    "#                     print(\"S_dgama\"+str(l), \" after = \", self.params[\"S_dgama\"+str(l)], \"\\n\")\n",
    "#                     print(\"S_dbeta\"+str(l), \" after = \", self.params[\"S_dbeta\"+str(l)], \"\\n\")\n",
    "                    \n",
    "                    \n",
    "            # \"Adam\" algorithm\n",
    "            else:\n",
    "                \n",
    "                # parameter used to update V_dW , V_db, S_dW and S_db\n",
    "                V_dW = self.params[\"V_dW\" + str(l)]\n",
    "                V_db = self.params[\"V_db\" + str(l)]\n",
    "                S_dW = self.params[\"S_dW\" + str(l)]\n",
    "                S_db = self.params[\"S_db\" + str(l)]\n",
    "                dW = self.grads[\"dW\" + str(l)]\n",
    "                db = self.grads[\"db\" + str(l)]\n",
    "                corr_factor_V = (1 - np.power(beta1, t))      # correction factor for V_dW and V_db\n",
    "                corr_factor_S = (1 - np.power(beta2, t))      # correction factor for S_dW and S_db\n",
    "                \n",
    "                # updating and storing V_dW, V_db, S_dW and S_db\n",
    "                self.params[\"V_dW\" + str(l)] = beta1*V_dW + (1-beta1)*dW\n",
    "                self.params[\"V_db\" + str(l)] = beta1*V_db + (1-beta1)*db\n",
    "                self.params[\"S_dW\" + str(l)] = beta2*S_dW + (1-beta2)*np.power(dW, 2)\n",
    "                self.params[\"S_db\" + str(l)] = beta2*S_db + (1-beta2)*np.power(db, 2)\n",
    "                \n",
    "                # corrected values of V_dW, V_db, S_dW and S_db\n",
    "                V_dW_corrected = self.params[\"V_dW\" + str(l)]/corr_factor_V\n",
    "                V_db_corrected = self.params[\"V_db\" + str(l)]/corr_factor_V\n",
    "                S_dW_corrected = self.params[\"S_dW\" + str(l)]/corr_factor_S\n",
    "                S_db_corrected = self.params[\"S_db\" + str(l)]/corr_factor_S\n",
    "                \n",
    "                # factors for updating W and b for neural layer\n",
    "                factor_dW = V_dW_corrected/(np.sqrt(S_dW_corrected) + epsilon)\n",
    "                factor_db = V_db_corrected/(np.sqrt(S_db_corrected) + epsilon)\n",
    "                \n",
    "#                 print(\"V_dW\"+str(l), \" before = \", V_dW, \"\\n\")\n",
    "#                 print(\"V_db\"+str(l), \" before = \", V_db, \"\\n\")\n",
    "#                 print(\"S_dW\"+str(l), \" before = \", S_dW, \"\\n\")\n",
    "#                 print(\"S_db\"+str(l), \" before = \", S_db, \"\\n\")\n",
    "#                 print(\"dW\"+str(l), \" = \", dW, \"\\n\")\n",
    "#                 print(\"db\"+str(l), \" = \", db, \"\\n\")\n",
    "#                 print(\"V_dW\" + str(l), \" after = \", self.params[\"V_dW\" + str(l)], \"\\n\")\n",
    "#                 print(\"V_db\" + str(l), \" after = \", self.params[\"V_db\" + str(l)], \"\\n\")\n",
    "#                 print(\"S_dW\"+str(l), \" after = \", self.params[\"S_dW\"+str(l)], \"\\n\")\n",
    "#                 print(\"S_db\"+str(l), \" after = \", self.params[\"S_db\"+str(l)], \"\\n\")\n",
    "#                 print(\"corr_factor_V = \", corr_factor_V, \"\\n\")\n",
    "#                 print(\"corr_factor_S = \", corr_factor_S, \"\\n\")\n",
    "#                 print(\"V_dW_corrected = \", V_dW_corrected, \"\\n\")\n",
    "#                 print(\"V_db_corrected = \", V_db_corrected, \"\\n\")\n",
    "#                 print(\"S_dW_corrected = \", S_dW_corrected, \"\\n\")\n",
    "#                 print(\"S_db_corrected = \", S_db_corrected, \"\\n\")\n",
    "                \n",
    "                # for batch_norm\n",
    "                if self.batch_norm == True:\n",
    "                    \n",
    "                    # parameter used for updating V_dgama, V_dbeta, S_dgama and S_dbeta\n",
    "                    V_dgama = self.params[\"V_dgama\" + str(l)]\n",
    "                    V_dbeta = self.params[\"V_dbeta\" + str(l)]\n",
    "                    S_dgama = self.params[\"S_dgama\" + str(l)]\n",
    "                    S_dbeta = self.params[\"S_dbeta\" + str(l)]\n",
    "                    d_gama = self.grads[\"d_gama\" + str(l)]\n",
    "                    d_beta = self.grads[\"d_beta\" + str(l)]\n",
    "                    \n",
    "                    # updating and storing V_dgama, V_dbeta, S_dgama and S_dbeta\n",
    "                    self.params[\"V_dgama\" + str(l)] = beta1*V_dgama + (1 - beta1)*d_gama\n",
    "                    self.params[\"V_dbeta\" + str(l)] = beta1*V_dbeta + (1 - beta1)*d_beta\n",
    "                    self.params[\"S_dgama\" + str(l)] = beta2*S_dgama + (1 - beta2)*np.power(d_gama, 2)\n",
    "                    self.params[\"S_dbeta\" + str(l)] = beta2*S_dbeta + (1 - beta2)*np.power(d_beta, 2)\n",
    "                    \n",
    "                    # corrected values of V_dgama, V_dbeta, S_dgama and S_dbeta\n",
    "                    V_dgama_corrected = self.params[\"V_dgama\" + str(l)]/corr_factor_V\n",
    "                    V_dbeta_corrected = self.params[\"V_dbeta\" + str(l)]/corr_factor_V\n",
    "                    S_dgama_corrected = self.params[\"S_dgama\" + str(l)]/corr_factor_S\n",
    "                    S_dbeta_corrected = self.params[\"S_dbeta\" + str(l)]/corr_factor_S\n",
    "\n",
    "                    # factors for updating d_gama and d_beta\n",
    "                    factor_dgama = V_dgama_corrected/(np.sqrt(S_dgama_corrected) + epsilon)\n",
    "                    factor_dbeta = V_dbeta_corrected/(np.sqrt(S_dbeta_corrected) + epsilon)\n",
    "                \n",
    "#                     print(\"V_dgama\"+str(l), \" before = \", V_dgama, \"\\n\")\n",
    "#                     print(\"V_dbeta\"+str(l), \" before = \", V_dbeta, \"\\n\")\n",
    "#                     print(\"V_dgama\"+str(l), \" after = \", self.params[\"V_dgama\"+str(l)], \"\\n\")\n",
    "#                     print(\"V_dbeta\"+str(l), \" after = \", self.params[\"V_dbeta\"+str(l)], \"\\n\")\n",
    "#                     print(\"d_gama\"+str(l), \" = \", d_gama, \"\\n\")\n",
    "#                     print(\"d_beta\"+str(l), \" = \", d_beta, \"\\n\")\n",
    "#                     print(\"S_dgama\"+str(l), \" before = \", S_dgama, \"\\n\")\n",
    "#                     print(\"S_dbeta\"+str(l), \" before = \", S_dbeta, \"\\n\")\n",
    "#                     print(\"S_dgama\"+str(l), \" after = \", self.params[\"S_dgama\"+str(l)], \"\\n\")\n",
    "#                     print(\"S_dbeta\"+str(l), \" after = \", self.params[\"S_dbeta\"+str(l)], \"\\n\")\n",
    "#                     print(\"d_gama\"+str(l), \" = \", d_gama, \"\\n\")\n",
    "#                     print(\"d_beta\"+str(l), \" = \", d_beta, \"\\n\")\n",
    "                    \n",
    "#                     print(\"V_dgama_corrected = \", V_dgama_corrected, \"\\n\")\n",
    "#                     print(\"V_dbeta_corrected = \", V_dbeta_corrected, \"\\n\")\n",
    "#                     print(\"S_dgama_corrected = \", S_dgama_corrected, \"\\n\")\n",
    "#                     print(\"S_dbeta_corrected = \", S_dbeta_corrected, \"\\n\")\n",
    "\n",
    "            \n",
    "#             print(\"factor_dW = \", factor_dW, \"\\n\")\n",
    "#             print(\"factor_db = \", factor_db, \"\\n\")\n",
    "#             print(\"W\"+str(l), \" before = \", self.params[\"W\"+str(l)], \"\\n\")    \n",
    "#             print(\"b\"+str(l), \" before = \", self.params[\"b\"+str(l)], \"\\n\")    \n",
    "            \n",
    "            self.params[\"W\"+str(l)] -= self.learning_rate*factor_dW\n",
    "            self.params[\"b\"+str(l)] -= self.learning_rate*factor_db\n",
    "            \n",
    "#             print(\"W\"+str(l), \" after = \", self.params[\"W\"+str(l)], \"\\n\")    \n",
    "#             print(\"b\"+str(l), \" after = \", self.params[\"b\"+str(l)], \"\\n\")    \n",
    "            \n",
    "            # updating for batch_norm\n",
    "            if self.batch_norm == True:\n",
    "                \n",
    "#                 print(\"factor_dgama = \", factor_dgama, \"\\n\")\n",
    "#                 print(\"factor_dbeta = \", factor_dbeta, \"\\n\")\n",
    "#                 print(\"gama\"+str(l), \" before = \", self.params[\"gama\"+str(l)], \"\\n\")    \n",
    "#                 print(\"beta\"+str(l), \" before = \", self.params[\"beta\"+str(l)], \"\\n\")    \n",
    "\n",
    "                self.params[\"gama\" + str(l)] -= self.learning_rate*factor_dgama\n",
    "                self.params[\"beta\" + str(l)] -= self.learning_rate*factor_dbeta\n",
    "                \n",
    "#                 print(\"gama\"+str(l), \" after = \", self.params[\"gama\"+str(l)], \"\\n\")    \n",
    "#                 print(\"beta\"+str(l), \" after = \", self.params[\"beta\"+str(l)], \"\\n\")    \n",
    "\n",
    "    \n",
    "    # training the model to get optimum values of Wl's and bl's\n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        '''\n",
    "        Parameters: self: \n",
    "                        l_dims, params_set, forward, cost_and_dAL, backward, update_params --> to train the model\n",
    "                        \n",
    "                    X --> whole training set\n",
    "                    Y --> true label set\n",
    "                    shape of X --> (no. of training examples, no. of features)\n",
    "                    shape of Y --> (no. of training examples, 1) -->  for \"reg\" or \"binary\" model\n",
    "                               --> (no. of training examples, no. of class) --> for \"multi\" model\n",
    "                    \n",
    "        '''\n",
    "        \n",
    "        X = X.T                                      # to change the shape of X to (no. of features , no. of training example)\n",
    "        \n",
    "        if self.model_type == \"multi\":\n",
    "            Y = Y.T.reshape(Y.shape[1], Y.shape[0])  # to change the shape of Y to (no. of class, no. of training ex.) \n",
    "            \n",
    "        else:\n",
    "            Y = Y.T.reshape(1, Y.shape[0])           # to change the shape to (1, no. of training example) \n",
    "        \n",
    "        n_x = X.shape[0]                             # no. of features of X or dimension of zeroth layer or input layer\n",
    "        self.l_dims.insert(0, n_x)                   # inserting the n_x at i = 0 position in l_dims list.\n",
    "        M = X.shape[1]                               # total no. of training example\n",
    "        self.Parameter_Initialization()\n",
    "        costs = []\n",
    "        epochs = []\n",
    "        t = 0\n",
    "#         print('shape of X = ', X.shape, \"\\n\")\n",
    "#         print(\"shape of Y = \", Y.shape, \"\\n\")\n",
    "        for epoch in range(self.max_epochs):\n",
    "            \n",
    "            seed = 1\n",
    "            mini_batches = self.Random_mini_batches(X, Y, seed)\n",
    "            cost_total = 0    # to store the cost for each pass/epoch\n",
    "\n",
    "            for batch in mini_batches:\n",
    "                \n",
    "                (mini_batch_X, mini_batch_Y) = batch\n",
    "#                 print(\"mini_batch_X = \", mini_batch_X, \"\\n\")\n",
    "#                 print(\"mini_batch_Y = \", mini_batch_Y, \"\\n\")\n",
    "                self.Forward_Prop(mini_batch_X)\n",
    "                \n",
    "                # updating mean_test and var_test for each mini_batch\n",
    "                # using exponentially weighted moving average when applied batch_norm\n",
    "                if self.batch_norm:\n",
    "                    \n",
    "                    self.Update_mean_and_var()\n",
    "                \n",
    "                cost_total += self.Cost_dAL_and_dZL(mini_batch_Y)\n",
    "                self.Back_Prop()\n",
    "                t = t+1  # parameter for \"Adam\" learning Algorithm\n",
    "                self.Parameter_Update(t)\n",
    "            \n",
    "            cost_avg = cost_total/M\n",
    "            \n",
    "            if self.is_decay:\n",
    "                \n",
    "                self.decay(epoch)\n",
    "            \n",
    "            # Print the cost every 1000 epoch\n",
    "            if epoch % 200 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" %(epoch, cost_avg))\n",
    "            if epoch % 100 == 0:\n",
    "                costs.append(cost_avg)\n",
    "                epochs.append(epoch)\n",
    "                \n",
    "        self.grad_checking(X, Y)\n",
    "        \n",
    "        return (costs, epochs)\n",
    "        \n",
    "    # predict function\n",
    "    def predict(self, X):\n",
    "\n",
    "        '''\n",
    "        Parameters:- self:\n",
    "                         params --> learned parameters from fit method\n",
    "                         X --> sample for which prediction is to be done\n",
    "                Return: Y_hat value for X\n",
    "        '''\n",
    "        X = X.T                   # to convert (# of sample, # of features) --> (# of features, # of samples)\n",
    "        M = X.shape[1]            # number of samples \n",
    "        L = len(self.l_dims) - 1  # index for output layer \n",
    "        self.Forward_Prop(X, function = \"predict\")      # Forward_prop to get the AL(post activation function value of output layer)\n",
    "        AL = self.activation_model[\"A\"+str(L)]\n",
    "\n",
    "        # for \"binary\" classification\n",
    "        if self.model_type == \"binary\":\n",
    "\n",
    "            # last layer activation function --> \"sigmoid\"\n",
    "            # if AL > 0.5 (threshold) --> y_hat = 1\n",
    "            # else --> y_hat = 0\n",
    "\n",
    "            AL[AL >= 0.5] = 1\n",
    "            AL[AL < 0.5] = 0\n",
    "            y_hat = AL.reshape((M, 1))\n",
    "\n",
    "        # for \"multi\" class classification\n",
    "        elif self.model_type == \"multi\":\n",
    "\n",
    "            # AL is of shape (# of classes, # of samples)\n",
    "            # get the index for the maximum value in each column, that will be the class\n",
    "            # AL[:,0] --> first column with c rows whose sum of values = 1 (probability)\n",
    "            # get the index of the maximum probabilty in the column, that will be our class\n",
    "\n",
    "            y_hat = np.argmax(AL, axis = 0).reshape((M, 1))\n",
    "\n",
    "        # for \"reg\" problem\n",
    "        else:\n",
    "\n",
    "            # AL values are your y_hat\n",
    "            # just reshape them\n",
    "\n",
    "            y_hat = AL.reshape((M, 1))\n",
    "\n",
    "        return y_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
